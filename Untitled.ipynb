{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "This code is heavily derived from word2veclite\n",
    "\n",
    "ref:\n",
    "1) https://github.com/cbellei/word2veclite\n",
    "2) http://www.claudiobellei.com/2018/01/07/backprop-word2vec-python/\n",
    "\"\"\"\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    \"\"\"\n",
    "    Tokenize the corpus of text.\n",
    "    :param corpus: list containing a string of text (example: [\"I like playing football with my friends\"])\n",
    "    :return corpus_tokenized: indexed list of words in the corpus, in the same order as the original corpus\n",
    "        (the example above would return [[1, 2, 3, 4]])\n",
    "    :return V: size of vocabulary\n",
    "    \"\"\"\n",
    "    # use for t-SNE visualization\n",
    "    global tokenizer\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    return tokenizer.texts_to_sequences(corpus), len(tokenizer.word_index)\n",
    "\n",
    "\n",
    "def initialize(V, N):\n",
    "    \"\"\"\n",
    "    Initialize the weights of the neural network.\n",
    "    :param V: size of the vocabulary\n",
    "    :param N: size of the hidden layer\n",
    "    :return: weights W1, W2\n",
    "    \"\"\"\n",
    "    np.random.seed(100)\n",
    "    W1 = np.random.rand(V, N)\n",
    "    W2 = np.random.rand(N, V)\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "\n",
    "def corpus2io(corpus_tokenized, V, window_size):\n",
    "    \"\"\"Converts corpus text into context and center words\n",
    "    # Arguments\n",
    "        corpus_tokenized: corpus text\n",
    "        window_size: size of context window\n",
    "    # Returns\n",
    "        context and center words (arrays)\n",
    "    \"\"\"\n",
    "    for words in corpus_tokenized:\n",
    "        w = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            center = []\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            contexts.append([words[i] - 1 for i in range(s, e) if 0 <= i < w and i != index])\n",
    "            center.append(word - 1)\n",
    "\n",
    "            contexts = contexts[0]  # IMPORTANT: dim reduction\n",
    "\n",
    "            x = np_utils.to_categorical(contexts, V)\n",
    "            y = np_utils.to_categorical(center, V)\n",
    "\n",
    "            yield (x, y.ravel())\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculate softmax based probability for given input vector\n",
    "    # Arguments\n",
    "        x: numpy array/list\n",
    "    # Returns\n",
    "        softmax of input array\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    \"\"\"\n",
    "    Python implementation of Word2Vec.\n",
    "\n",
    "    # Arguments\n",
    "        method : `str`\n",
    "            choose method for word2vec (options: 'cbow', 'skipgram')\n",
    "            [default: 'cbow']\n",
    "        window_size: `integer`\n",
    "            size of window [default: 1]\n",
    "        n_hidden: `integer`\n",
    "            size of hidden layer [default: 2]\n",
    "        n_epochs: `integer`\n",
    "            number of epochs [default: 1]\n",
    "        learning_rate: `float` [default: 0.1]\n",
    "        corpus: `str`\n",
    "            corpus text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='cbow', window_size=1, n_hidden=2, n_epochs=1, corpus='', learning_rate=0.1):\n",
    "        self.window = window_size\n",
    "        self.N = n_hidden\n",
    "        self.n_epochs = n_epochs\n",
    "        self.corpus = corpus\n",
    "        self.eta = learning_rate\n",
    "        if method == 'cbow':\n",
    "            self.method = self.cbow\n",
    "        elif method == 'skipgram':\n",
    "            self.method = self.skipgram\n",
    "        else:\n",
    "            raise ValueError(\"Method not recognized. Aborting.\")\n",
    "\n",
    "    def cbow(self, context, center, W1, W2, loss):\n",
    "        \"\"\"\n",
    "        Implementation of Continuous-Bag-of-Words Word2Vec model\n",
    "        :param context: all the context words (these represent the inputs)\n",
    "        :param center: the center word (this represents the label)\n",
    "        :param W1: weights from the input to the hidden layer\n",
    "        :param W2: weights from the hidden to the output layer\n",
    "        :param loss: float that represents the current value of the loss function\n",
    "        :return: updated weights and loss\n",
    "        \"\"\"\n",
    "        x = np.mean(context, axis=0)\n",
    "        h = np.dot(W1.T, x)\n",
    "        u = np.dot(W2.T, h)\n",
    "        y_pred = softmax(u)\n",
    "\n",
    "        e = -center + y_pred\n",
    "\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.outer(x, np.dot(W2, e))\n",
    "\n",
    "        new_W1 = W1 - self.eta * dW1\n",
    "        new_W2 = W2 - self.eta * dW2\n",
    "\n",
    "        loss += -float(u[center == 1]) + np.log(np.sum(np.exp(u)))\n",
    "\n",
    "        return new_W1, new_W2, loss\n",
    "\n",
    "    def skipgram(self, context, center, W1, W2, loss):\n",
    "        \"\"\"\n",
    "        Implementation of Skip-Gram Word2Vec model\n",
    "        :param context: all the context words (these represent the labels)\n",
    "        :param center: the center word (this represents the input)\n",
    "        :param W1: weights from the input to the hidden layer\n",
    "        :param W2: weights from the hidden to the output layer\n",
    "        :param loss: float that represents the current value of the loss function\n",
    "        :return: updated weights and loss\n",
    "        \"\"\"\n",
    "        h = np.dot(W1.T, center)\n",
    "        u = np.dot(W2.T, h)\n",
    "        y_pred = softmax(u)\n",
    "\n",
    "        e = np.array([-label + y_pred.T for label in context])\n",
    "\n",
    "        dW2 = np.outer(h, np.sum(e, axis=0))\n",
    "        dW1 = np.outer(center, np.dot(W2, np.sum(e, axis=0).T))\n",
    "\n",
    "        new_W1 = W1 - self.eta * dW1\n",
    "        new_W2 = W2 - self.eta * dW2\n",
    "\n",
    "        loss += -2 * np.log(len(context)) \\\n",
    "                - np.sum([u[label == 1] for label in context]) + len(context) * np.log(np.sum(np.exp(u)))\n",
    "\n",
    "        return new_W1, new_W2, loss\n",
    "\n",
    "    def predict(self, x, W1, W2):\n",
    "        \"\"\"Predict output from input data and weights\n",
    "        :param x: input data\n",
    "        :param W1: weights from input to hidden layer\n",
    "        :param W2: weights from hidden layer to output layer\n",
    "        :return: output of neural network\n",
    "        \"\"\"\n",
    "        h = np.mean([np.dot(W1.T, xx) for xx in x], axis=0)\n",
    "        u = np.dot(W2.T, h)\n",
    "\n",
    "        return softmax(u)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Main method of the Word2Vec class.\n",
    "        :return: the final values of the weights W1, W2 and a history of the value of the loss function vs. epoch\n",
    "        \"\"\"\n",
    "        if len(self.corpus) == 0:\n",
    "            raise ValueError('You need to specify a corpus of text.')\n",
    "\n",
    "        corpus_tokenized, V = tokenize(self.corpus)\n",
    "        W1, W2 = initialize(V, self.N)\n",
    "\n",
    "        loss_vs_epoch = []\n",
    "        for e in range(self.n_epochs):\n",
    "            loss = 0.\n",
    "            for context, center in corpus2io(corpus_tokenized, V, self.window):\n",
    "                W1, W2, loss = self.method(context, center, W1, W2, loss)\n",
    "            loss_vs_epoch.append(loss)\n",
    "\n",
    "        return W1, W2, loss_vs_epoch\n",
    "\n",
    "    \n",
    "corpus = [\n",
    "    \"I like playing football with my friends\",\n",
    "    \"I like football\",\n",
    "    \"I like soccer\",\n",
    "    \"I like playing soccer\",\n",
    "    \"I like playing with my friends\",\n",
    "    \"I like my friends\",\n",
    "    \"I like friends\",\n",
    "    \"football with friends\",\n",
    "    \"soccer with friends\",\n",
    "]\n",
    "\n",
    "# corpus = [\n",
    "#     \"나는 정말 너가 좋다\",\n",
    "#     \"오늘은 행복한 날인 것 같다.\",\n",
    "#     \"누군가에게 따뜻함을 줄 수 있다는 것만큼 아름다운 것은 없다.\",\n",
    "#     \"나는 좆또 나은점이 없지만, 너가 내 옆에 있다는 것으로 충분해.\",\n",
    "#     \"너를 사랑해, 사실 거짓말이야\",\n",
    "# ]\n",
    "\n",
    "w2v = Word2Vec(method=\"cbow\", corpus=corpus,\n",
    "               window_size=2, n_hidden=5,\n",
    "               n_epochs=100, learning_rate=0.1)\n",
    "W1, W2, loss_vs_epoch = w2v.run()\n",
    "\n",
    "# %%\n",
    "plt.plot(loss_vs_epoch)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rc('font', family='AppleGothic')\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "low_dim_embs = tsne.fit_transform(W1)\n",
    "for i in range(0, len(tokenizer.word_index)):\n",
    "    c = 0\n",
    "    for k in tokenizer.word_index:\n",
    "        if i == c:\n",
    "            break\n",
    "        c += 1\n",
    "    plt.scatter(low_dim_embs[i, 0], low_dim_embs[i, 1])\n",
    "    plt.annotate(k,\n",
    "                 xy=(low_dim_embs[i, 0], low_dim_embs[i, 1]))\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 0, 0, 7, 5, 3, 2, 1]\n",
      "(17,)\n"
     ]
    }
   ],
   "source": [
    "list1 = [1,2,3,4,5,6,7,8,9,9,0,0,7,5,3,2,1]\n",
    "list2 = np.array(list1)\n",
    "print(list1)\n",
    "print(list2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3 = [[1,2,3,4,5,6,7,8,9,9,0,0,7,5,3,2,1],[1,2,3,4,5,6,7,8,9,9,0,0,7,5,3,2,1]]\n",
    "list4 = np.matrix(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 0, 0, 7, 5, 3, 2, 1], [1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 0, 0, 7, 5, 3, 2, 1]]\n",
      "[[1 2 3 4 5 6 7 8 9 9 0 0 7 5 3 2 1]\n",
      " [1 2 3 4 5 6 7 8 9 9 0 0 7 5 3 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print(list3)\n",
    "print(list4)\n",
    "list5 = np.array(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5 6 7 8 9 9 0 0 7 5 3 2 1]\n",
      " [1 2 3 4 5 6 7 8 9 9 0 0 7 5 3 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print(list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1,0,0,0,0,0,0,1]\n",
    "y_test = np_utils.to_categorical(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24/12:45:50'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(now.day)+\"/\"+str(now.hour)+\":\"+str(now.minute)+\":\"+str(now.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [1,0,0,0,0,0,0,1].extend([7]*1)\n",
    "a = [1,0,0,0,0,0,0,1]\n",
    "a.extend(['0']*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
