{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_array(6651, 13)\n",
      "val_array(887, 13)\n",
      "test_array(3239, 13)\n",
      "Reading data!\n",
      "checking Anomaly!!\n",
      "(6619,)\n",
      "(6619, 11)\n",
      "(883,)\n",
      "(883, 11)\n",
      "(3215,)\n",
      "(3215, 11)\n",
      "ANomaly result!\n",
      "x_train.shape(6619,)\n",
      "y_train.shape(6619, 11)\n",
      "x_val.shape(883,)\n",
      "y_val.shape(883, 11)\n",
      "x_test.shape(3215,)\n",
      "y_test.shape(3215, 11)\n",
      "Finished!\n",
      "Tokenizing!\n",
      "Tokenizing finished!\n",
      "It's by len()\n",
      "train_tmp.len():6619,40\n",
      "val_tmp shape.len():883,39\n",
      "test_tmp shape.len():3215,46\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, re, csv, math, codecs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer as t\n",
    "from tqdm import tqdm\n",
    "from keras.layers import merge, Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import *\n",
    "# to visualize, and to make zero shape matrix\n",
    "from attention_utils import get_activations, get_data_recurrent\n",
    "from Attention import Attention\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt as Twitter\n",
    "from selfword2vec import tokenization\n",
    "from Anomaly import checkAnomaly_x_y\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from numpy import argmax\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# 1. 모델 저장시키기 \n",
    "# 2. tokenizer와 konlpy morphs 호환 여부 (완성 jupyter note)\n",
    "# 2'. word2vec 2가지 경우 더 추가 to embedding(final_total word2vec, twitter_translated.vec)\n",
    "# 3. 변수들 설정하기\n",
    "\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# NUM_WORDS, train으로 input받은 단어의 수\n",
    "MAX_NB_WORDS = 20000\n",
    "vocab_size = 0\n",
    "EMB_DIM = 300\n",
    "embeddings_index = dict()\n",
    "\n",
    "# columns = [\"ID\", \"Tweet\", \"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\",\"sadness\",\"surprise\",\"trust\"]\n",
    "columns = [\"ID\",\"Tweet\",\"분노\",\"기대\",\"혐오스러운\",\"두려움\",\"기쁨\",\"사랑\",\"낙관론\",\"비관론\",\"슬픔\",\"놀라움\",\"믿음\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################1. 데이터셋 생성하기\n",
    "\n",
    "train_array = pd.read_csv(\"/home/minwookje/coding/ex1_w2v/data/tweet/dump/kor_train.txt\",sep=\"\\t\", header=None,names=columns).values\n",
    "val_array = pd.read_csv(\"/home/minwookje/coding/ex1_w2v/data/tweet/dump/kor_dev.txt\",sep=\"\\t\", header=None,names=columns).values\n",
    "test_array = pd.read_csv(\"/home/minwookje/coding/ex1_w2v/data/tweet/dump/kor_test_gold.txt\",sep=\"\\t\", header=None,names=columns).values\n",
    "\n",
    "# 판다 shape\n",
    "print(\"train_array\"+ str(train_array.shape))\n",
    "print(\"val_array\"+str(val_array.shape))\n",
    "print(\"test_array\"+str(test_array.shape))\n",
    "\n",
    "print(\"Reading data!\")\n",
    "#  x, y 분할하기\n",
    "x_train = train_array[1:,1]\n",
    "y_train = train_array[1:,2:]\n",
    "x_val = val_array[1:,1]\n",
    "y_val = val_array[1:,2:]\n",
    "x_test = test_array[1:,1]\n",
    "y_test = test_array[1:,2:]\n",
    "\n",
    "\n",
    "print(\"checking Anomaly!!\")\n",
    "x_train, y_train = checkAnomaly_x_y(x_train,y_train)\n",
    "x_val, y_val = checkAnomaly_x_y(x_val,y_val)\n",
    "x_test, y_test = checkAnomaly_x_y(x_test,y_test)\n",
    "\n",
    "print(\"ANomaly result!\")\n",
    "print(\"x_train.shape\"+ str(x_train.shape))\n",
    "print(\"y_train.shape\"+ str(y_train.shape))\n",
    "print(\"x_val.shape\"+ str(x_val.shape))\n",
    "print(\"y_val.shape\"+ str(y_val.shape))\n",
    "print(\"x_test.shape\"+ str(x_test.shape))\n",
    "print(\"y_test.shape\"+ str(y_test.shape))\n",
    "# print(type(x_train)) np.array로 변형이 필요한가\n",
    "print(\"Finished!\")\n",
    "\n",
    "# jupyter notebook\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "tmp = []\n",
    "train_tmp = []\n",
    "val_tmp = []\n",
    "test_tmp = []\n",
    "max_count = 0\n",
    "set_words = set()\n",
    "\n",
    "print(\"Tokenizing!\")\n",
    "# test의 tmp만 따로 받아주고 나머지는 val train test모두 통합시켜준다.\n",
    "test_tmp, dummy , set_words = tokenization(x_test)\n",
    "val_tmp, dummy , set_words = tokenization(x_val)\n",
    "train_tmp, max_count , set_words = tokenization(x_train)\n",
    "tmp, dummy , set_words = tokenization(np.hstack([x_train,x_val,x_test]))\n",
    "print(\"Tokenizing finished!\")\n",
    "tmp = [] # tmp없애준다\n",
    "\n",
    "\n",
    "\n",
    "#token shape\n",
    "print(\"It's by len()\")\n",
    "print(\"train_tmp.len():\" + str(len(train_tmp))+\",\"+ str(len(train_tmp[0])))\n",
    "print(\"val_tmp shape.len():\" + str(len(val_tmp))+\",\"+ str(len(val_tmp[0])))\n",
    "print(\"test_tmp shape.len():\" + str(len(test_tmp))+\",\"+ str(len(test_tmp[0])))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "629it [00:00, 6284.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_count:10531\n",
      "Readding Embedding file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45942it [00:06, 7119.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45942 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"max_count:\"+str(max_count))\n",
    "# 문장길이 100으로 맞춘다.\n",
    "max_count = min(100, max_count)\n",
    "\n",
    "\n",
    "print(\"Readding Embedding file\")\n",
    "# embeddings_index == dict(w2v's word: vector)\n",
    "f = codecs.open('/home/minwookje/coding/ex1_w2v/embedding/1542954106final_total_pos.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vec shape:6619,100\n",
      "word_vec_val shape:883,100\n",
      "word_vec_test shape:3215,100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# word2index table 생성 {token:index}\n",
    "# 모든 train val test에 사용되는 token별 index table\n",
    "# train 문장 token 갯수\n",
    "word_index = len(set_words)\n",
    "word_tmp_dict = dict()\n",
    "for i, word in enumerate(set_words):\n",
    "    word_tmp_dict[str(word).replace(\" \", \"\")] = i\n",
    "word_tmp_dict['0'] = word_index \n",
    "\n",
    "## 문장별 토큰화시킨 녀석에 index를 집어 넣어준다. 이때 pad도 동시에 해준다. \n",
    "word_vec = []\n",
    "word_vec_test = []\n",
    "word_vec_val = []\n",
    "\n",
    "for sent in train_tmp:\n",
    "    sub = []\n",
    "    for word in sent:\n",
    "        if(len(sub)==max_count):\n",
    "            break    \n",
    "         #print(word)\n",
    "         #print(type(str(word)))\n",
    "         #break\n",
    "# word는 tuple 타입, embeddings_index는 str타입, tuple 타입을 str()화시키면 \n",
    "# 중간에 space가 생성되어 match가 되지 않았다. 이를 해결해주었다. \n",
    "        if(str(word).replace(\" \", \"\") in word_tmp_dict):\n",
    "            \n",
    "            sub.append(word_tmp_dict[str(word).replace(\" \", \"\")])\n",
    "        else:\n",
    "            print(\"sentence index화 실패\")\n",
    "    count = max_count - len(sub)\n",
    "    # padding\n",
    "    sub.extend([word_index]*count)\n",
    "    word_vec.append(sub)\n",
    "## 테스트용 복사본 \n",
    "for sent in test_tmp:\n",
    "    sub = []\n",
    "    for word in sent:\n",
    "        if(len(sub)==max_count):\n",
    "            break\n",
    "         #print(word)\n",
    "         #print(type(str(word)))\n",
    "         #break\n",
    "# word는 tuple 타입, embeddings_index는 str타입, tuple 타입을 str()화시키면 \n",
    "# 중간에 space가 생성되어 match가 되지 않았다. 이를 해결해주었다. \n",
    "        if(str(word).replace(\" \", \"\") in word_tmp_dict):\n",
    "            \n",
    "            sub.append(word_tmp_dict[str(word).replace(\" \", \"\")])\n",
    "        else:\n",
    "            print(\"sentence index화 실패\")\n",
    "    count = max_count - len(sub)\n",
    "    # padding\n",
    "    sub.extend([word_index]*count)\n",
    "    word_vec_test.append(sub)\n",
    "\n",
    "## 검증용 복사본 \n",
    "for sent in val_tmp:\n",
    "    sub = []\n",
    "    for word in sent:\n",
    "        if(len(sub)==max_count):\n",
    "            break\n",
    "         #print(word)\n",
    "         #print(type(str(word)))\n",
    "         #break\n",
    "# word는 tuple 타입, embeddings_index는 str타입, tuple 타입을 str()화시키면 \n",
    "# 중간에 space가 생성되어 match가 되지 않았다. 이를 해결해주었다. \n",
    "        if(str(word).replace(\" \", \"\") in word_tmp_dict):\n",
    "            \n",
    "            sub.append(word_tmp_dict[str(word).replace(\" \", \"\")])\n",
    "        else:\n",
    "            print(\"sentence index화 실패\")\n",
    "    count = max_count - len(sub)\n",
    "    # padding\n",
    "    sub.extend([word_index]*count)\n",
    "    word_vec_val.append(sub)\n",
    "\n",
    "print(\"word_vec shape:\" + str(len(word_vec))+\",\"+ str(len(word_vec[0])))\n",
    "print(\"word_vec_val shape:\" + str(len(word_vec_val))+\",\"+ str(len(word_vec_val[0])))\n",
    "print(\"word_vec_test shape:\" + str(len(word_vec_test))+\",\"+ str(len(word_vec_test[0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print((type(word_vec_test[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match:11300\n",
      "unmatch:12627\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 4번쨰 matrix embedding_matrix {index: vector}\n",
    "# vocab_size = min(MAX_NB_WORDS, word_index)\n",
    "vocab_size = word_index\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size+1, EMB_DIM))\n",
    "match_count = 0\n",
    "unmatch_count = 0\n",
    "mins = []\n",
    "maxs = []\n",
    "for word, i in word_tmp_dict.items():\n",
    "    # if word != '0':\n",
    "    if (word in embeddings_index):\n",
    "        match_count += 1\n",
    "        \n",
    "        # embedding_matrix[i] = np.zeros(300)\n",
    "#         embedding_matrix[i] = embeddings_index[word]\n",
    "        embedding_matrix[i] = embeddings_index[word].astype('float32')\n",
    "#         embedding_matrix[i] = np.random.uniform(-1.0,1.0,300).astype('float64')\n",
    "        for i in embedding_matrix[i]:\n",
    "            if i <= -1:\n",
    "                mins.append(i)\n",
    "            elif i >= 1:\n",
    "                maxs.append(i)    \n",
    "                \n",
    "#         embedding_matrix[i] = np.random.uniform(-0.25,0.25,300)\n",
    "    else:\n",
    "        unmatch_count += 1\n",
    "        # embedding_matrix[i] = np.zeros(300)\n",
    "        # embedding_matrix[i] = np.random.uniform(-0.25,0.25,300) ## used for OOV words\n",
    "        embedding_matrix[i] = np.random.uniform(-1.0,1.0,300).astype('float32')\n",
    "print(\"match:\" + str(match_count))\n",
    "print(\"unmatch:\" + str(unmatch_count))\n",
    "#     여기부터\n",
    "# 1.앞서 단어당 벡터 테이블(v) // embeddings_index, {w2v_word: vector}\n",
    "# train_word(str(word).replace(\" \", \"\")) == embedding\n",
    "# 2.train 단어별 index (v) // word_tmp_dict {train_word(str(word).replace(\" \", \"\")):index}\n",
    "# 3.sentence padding, sentence to index\n",
    "# 4.index당 vector table (v) //embedding_matrix {index: vector} 이녀석을 embedding weight에 넣어주어야 한다. \n",
    "\n",
    "# 문장 = [index들 나열 ] \n",
    "# 즉 embedding_matrix로 index를 seq에 넣어준묹장들을 train에 넣어줘야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vec shape:6619,100\n",
      "word_vec_val shape:883,100\n",
      "word_vec_test shape:3215,100\n",
      "word_vec shape:(6619, 100)\n",
      "word_vec_val shape:(883, 100)\n",
      "word_vec_test shape:(3215, 100)\n",
      "y_train shape:(6619, 11)\n",
      "y_val shape:(883, 11)\n",
      "y_test shape:(3215, 11)\n",
      "embedding_matrix shape:(23927, 300)\n"
     ]
    }
   ],
   "source": [
    "# print(mins)\n",
    "# print(maxs)\n",
    "# np.array화\n",
    "print(\"word_vec shape:\" + str(len(word_vec))+\",\"+ str(len(word_vec[0])))\n",
    "print(\"word_vec_val shape:\" + str(len(word_vec_val))+\",\"+ str(len(word_vec_val[0])))\n",
    "print(\"word_vec_test shape:\" + str(len(word_vec_test))+\",\"+ str(len(word_vec_test[0])))\n",
    "\n",
    "word_vec = np.array(word_vec)\n",
    "word_vec_val = np.array(word_vec_val)\n",
    "word_vec_test = np.array(word_vec_test)\n",
    "embedding_matrix = embedding_matrix\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"word_vec shape:\" + str(word_vec.shape))\n",
    "print(\"word_vec_val shape:\" + str(word_vec_val.shape))\n",
    "print(\"word_vec_test shape:\" + str(word_vec_test.shape))\n",
    "print(\"y_train shape:\" + str(y_train.shape))\n",
    "print(\"y_val shape:\" + str(y_val.shape))\n",
    "print(\"y_test shape:\" + str(y_test.shape))\n",
    "print(\"embedding_matrix shape:\" + str(embedding_matrix.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k,v in embeddings_index.items():\n",
    "    print(type(v[0]))\n",
    "    break\n",
    "    \n",
    "type(embedding_matrix[word_vec[0][0]][0])\n",
    "# type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def jaccard_distance_acc(y_true, y_pred):\n",
    "#     intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "#     sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "#     jac = (intersection) / (sum_ - intersection)\n",
    "#     return jac\n",
    "\n",
    "INPUT_DIM = 300 #wordvec사이즈\n",
    "max_count = max_count #문장의 max길이\n",
    "lstm_shape = 250\n",
    "rate_drop_lstm = 0.15\n",
    "rate_drop_dense = 0.15\n",
    "\n",
    "k_vec = word_vec\n",
    "k_vec_val = word_vec_val\n",
    "k_vec_test = word_vec_test\n",
    "k_y_train = y_train\n",
    "k_y_val = y_val\n",
    "k_y_test = y_test\n",
    "k_embedding_matrix = embedding_matrix\n",
    "\n",
    "# nan delete\n",
    "index = np.argwhere(np.isnan(k_y_train))[:,0]\n",
    "index2 = np.argwhere(np.isnan(k_y_val))[:,0]\n",
    "index3 = np.argwhere(np.isnan(k_y_test))[:,0]\n",
    "\n",
    "k_y_train = np.delete(k_y_train,index,0)\n",
    "k_vec = np.delete(k_vec,index,0)\n",
    "k_y_val = np.delete(k_y_val,index2,0)\n",
    "k_vec_val = np.delete(k_vec_val,index2,0)\n",
    "k_vec_test = np.delete(k_vec_test,index3,0)\n",
    "k_y_test = np.delete(k_y_test,index3,0)\n",
    "# is nan check\n",
    "print(np.any(np.isnan(k_vec)))\n",
    "print(np.any(np.isnan(k_vec_val)))\n",
    "print(np.any(np.isnan(k_vec_test)))\n",
    "print(np.any(np.isnan(k_y_train)))\n",
    "print(np.any(np.isnan(k_y_val)))\n",
    "print(np.any(np.isnan(k_y_test)))\n",
    "print(np.any(np.isnan(k_embedding_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 867 samples\n",
      "Epoch 1/10\n",
      "6474/6474 [==============================] - 110s 17ms/step - loss: 0.5135 - jaccard_distance_acc: 0.1627 - val_loss: 0.4818 - val_jaccard_distance_acc: 0.1514\n",
      "Epoch 2/10\n",
      "6474/6474 [==============================] - 110s 17ms/step - loss: 0.4730 - jaccard_distance_acc: 0.1610 - val_loss: 0.4803 - val_jaccard_distance_acc: 0.1563\n",
      "Epoch 3/10\n",
      "6474/6474 [==============================] - 120s 18ms/step - loss: 0.4713 - jaccard_distance_acc: 0.1608 - val_loss: 0.4754 - val_jaccard_distance_acc: 0.1636\n",
      "Epoch 4/10\n",
      "6474/6474 [==============================] - 119s 18ms/step - loss: 0.4705 - jaccard_distance_acc: 0.1609 - val_loss: 0.4745 - val_jaccard_distance_acc: 0.1701\n",
      "Epoch 5/10\n",
      "6474/6474 [==============================] - 112s 17ms/step - loss: 0.4704 - jaccard_distance_acc: 0.1619 - val_loss: 0.4743 - val_jaccard_distance_acc: 0.1675\n",
      "Epoch 6/10\n",
      "6474/6474 [==============================] - 104s 16ms/step - loss: 0.4702 - jaccard_distance_acc: 0.1623 - val_loss: 0.4762 - val_jaccard_distance_acc: 0.1628\n",
      "Epoch 7/10\n",
      "5120/6474 [======================>.......] - ETA: 22s - loss: 0.4700 - jaccard_distance_acc: 0.1633"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Concatenate, Permute, SpatialDropout1D, RepeatVector, LSTM, Bidirectional, Multiply, Lambda, Dense, Dropout, Input,Flatten,Embedding\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = tf.convert_to_tensor(1e-7, dtype='float32')\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection) / (sum_ - intersection +epsilon)\n",
    "    return (1 - jac)\n",
    "\n",
    "# matrix accu\n",
    "def jaccard_distance_acc(y_true, y_pred):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection) / (sum_ - intersection)\n",
    "    return jac\n",
    "accu = jaccard_distance_acc    \n",
    "\n",
    "# 32하니까 죽어버린다.\n",
    "# 512\n",
    "batch_size = 512\n",
    "inp = Input(shape=(max_count,), dtype='float32')\n",
    "emb = Embedding(input_dim=vocab_size+1, output_dim=EMB_DIM,\n",
    "            trainable=False, weights=[embedding_matrix], input_length=max_count)(inp)\n",
    "# max_features = vocab_size, maxlen=text_max_words, embed_size=EMB_DIM\n",
    "# emb = Embedding(input_dim=max_features, input_length = maxlen, output_dim=embed_size)(inp)\n",
    "# embedding dropout = 0.1\n",
    "x = SpatialDropout1D(0.1)(emb)\n",
    "x = Bidirectional(LSTM(lstm_shape, return_sequences=True, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))(x)\n",
    "repeat_size = int(x.shape[-1])\n",
    "x_a = Dense(repeat_size, kernel_initializer = 'glorot_uniform', activation=\"tanh\", name=\"tanh_mlp\")(x)\n",
    "x_a= Dense(1, kernel_initializer = 'glorot_uniform', activation='linear', name=\"word-level_context\")(x_a)\n",
    "x_a= Flatten()(x_a)\n",
    "att_out = Activation('softmax')(x_a) \n",
    "x_a2 = RepeatVector(repeat_size)(att_out)\n",
    "x_a2 = Permute([2,1])(x_a2)\n",
    "out = Multiply()([x,x_a2])\n",
    "# x = Dense(11, activation=\"sigmoid\")(x)\n",
    "out = Lambda(lambda x : K.sum(x, axis=1), name='expectation_over_words')(out)\n",
    "out = Dense(11, activation=\"sigmoid\")(out)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "\n",
    "# model.compile(loss=jaccard_distance_loss,\n",
    "#             optimizer='adam',\n",
    "#             metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=[accu])\n",
    "# intermediate_output = model.predict(k_vec)\n",
    "model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=10, verbose=1, batch_size=batch_size)\n",
    "# # model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=1, verbose=1,batch_size=None)\n",
    "\n",
    "\n",
    "# # model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=3, verbose=1, steps_per_epoch=int(6619/batch_size)+1, validation_steps = int(883/batch_size)+1)\n",
    "\n",
    "# # 5. 모델 평가하기\n",
    "score, acc = model.evaluate(k_vec_test, k_y_test, batch_size=batch_size)\n",
    "# # score, acc = model.evaluate(k_vec_test, k_y_test,batch_size=None)\n",
    "print('')\n",
    "print('score : ' + str(score)+ 'acc : ' + str(acc))\n",
    "\n",
    "\n",
    "# # loss = tf.nn.sigmoid_cross_entropy_with_logits\n",
    "# # loss = binary_crossentropy\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#             optimizer='adam',\n",
    "#             metrics=['accuracy'])\n",
    "# attention_model = Model(inputs=inp, outputs=attention) # Model to print out the attention data\n",
    "model.summary()\n",
    "# verbose= ? , validation_split은 validation file로 변환시켜주어야 한다.\n",
    "\n",
    "\n",
    "\n",
    "# h = model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=1, verbose=1, batch_size=batch_size)\n",
    "# # model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=1, verbose=1,batch_size=None)\n",
    "\n",
    "\n",
    "# # model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=3, verbose=1, steps_per_epoch=int(6619/batch_size)+1, validation_steps = int(883/batch_size)+1)\n",
    "\n",
    "# # 5. 모델 평가하기\n",
    "# score, acc = model.evaluate(k_vec_test, k_y_test, batch_size=batch_size)\n",
    "# # score, acc = model.evaluate(k_vec_test, k_y_test,batch_size=None)\n",
    "# print('')\n",
    "# print('score : ' + str(score)+ 'acc : ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_states = K.variable(value=np.random.normal(size=(100, 500)))\n",
    "# cell_states = K.variable(value=np.random.normal(size=(100, 500)))\n",
    "\n",
    "# model.layers[3].states[0] = hidden_states\n",
    "# model.layers[3].states[1] = cell_states \n",
    "\n",
    "model = h.model  # include here your original model\n",
    "k = np.array(list(k_vec))\n",
    "layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('bidirectional_5').get_input_at(0))\n",
    "intermediate_output = intermediate_layer_model.predict(k)\n",
    "\n",
    "\n",
    "# # with a Sequential model\n",
    "# get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "#                                   [model.layers[1].output])\n",
    "# layer_output = get_3rd_layer_output([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 100, 300)     7178100     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_11 (SpatialDr (None, 100, 300)     0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 100, 500)     1102000     spatial_dropout1d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tanh_mlp (Dense)                (None, 100, 500)     250500      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "word-level_context (Dense)      (None, 100, 1)       501         tanh_mlp[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 100)          0           word-level_context[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 100)          0           flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_10 (RepeatVector) (None, 500, 100)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_10 (Permute)            (None, 100, 500)     0           repeat_vector_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 100, 500)     0           bidirectional_11[0][0]           \n",
      "                                                                 permute_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expectation_over_words (Lambda) (None, 500)          0           multiply_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 11)           5511        expectation_over_words[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 8,536,612\n",
      "Trainable params: 1,358,512\n",
      "Non-trainable params: 7,178,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Concatenate, Permute, SpatialDropout1D, RepeatVector, LSTM, Bidirectional, Multiply, Lambda, Dense, Dropout, Input,Flatten,Embedding\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "# 32하니까 죽어버린다.\n",
    "# 512\n",
    "batch_size = 32\n",
    "inp = Input(shape=(max_count,), dtype='float64')\n",
    "emb = Embedding(input_dim=vocab_size+1, output_dim=EMB_DIM,\n",
    "            trainable=False, weights=[embedding_matrix], input_length=max_count)(inp)\n",
    "# max_features = vocab_size, maxlen=text_max_words, embed_size=EMB_DIM\n",
    "# emb = Embedding(input_dim=max_features, input_length = maxlen, output_dim=embed_size)(inp)\n",
    "# embedding dropout = 0.1\n",
    "x = SpatialDropout1D(0.1)(emb)\n",
    "x = Bidirectional(LSTM(lstm_shape, return_sequences=True, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))(x)\n",
    "repeat_size = int(x.shape[-1])\n",
    "x_a = Dense(repeat_size, kernel_initializer = 'glorot_uniform', activation=\"tanh\", name=\"tanh_mlp\")(x)\n",
    "x_a= Dense(1, kernel_initializer = 'glorot_uniform', activation='linear', name=\"word-level_context\")(x_a)\n",
    "x_a= Flatten()(x_a)\n",
    "att_out = Activation('softmax')(x_a) \n",
    "x_a2 = RepeatVector(repeat_size)(att_out)\n",
    "x_a2 = Permute([2,1])(x_a2)\n",
    "out = Multiply()([x,x_a2])\n",
    "# x = Dense(11, activation=\"sigmoid\")(x)\n",
    "out = Lambda(lambda x : K.sum(x, axis=1), name='expectation_over_words')(out)\n",
    "out = Dense(11, activation=\"sigmoid\")(out)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#             optimizer='adam',\n",
    "#             metrics=['accuracy'])\n",
    "intermediate_output = model.predict(k_vec)\n",
    "# h = model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=1, verbose=1, batch_size=batch_size)\n",
    "# # model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=1, verbose=1,batch_size=None)\n",
    "\n",
    "\n",
    "# # model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=3, verbose=1, steps_per_epoch=int(6619/batch_size)+1, validation_steps = int(883/batch_size)+1)\n",
    "\n",
    "# # 5. 모델 평가하기\n",
    "# score, acc = model.evaluate(k_vec_test, k_y_test, batch_size=batch_size)\n",
    "# # score, acc = model.evaluate(k_vec_test, k_y_test,batch_size=None)\n",
    "# print('')\n",
    "# print('score : ' + str(score)+ 'acc : ' + str(acc))\n",
    "\n",
    "\n",
    "# # loss = tf.nn.sigmoid_cross_entropy_with_logits\n",
    "# # loss = binary_crossentropy\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#             optimizer='adam',\n",
    "#             metrics=['accuracy'])\n",
    "# attention_model = Model(inputs=inp, outputs=attention) # Model to print out the attention data\n",
    "model.summary()\n",
    "# verbose= ? , validation_split은 validation file로 변환시켜주어야 한다.\n",
    "\n",
    "\n",
    "\n",
    "# h = model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=1, verbose=1, batch_size=batch_size)\n",
    "# # model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=1, verbose=1,batch_size=None)\n",
    "\n",
    "\n",
    "# # model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=3, verbose=1, steps_per_epoch=int(6619/batch_size)+1, validation_steps = int(883/batch_size)+1)\n",
    "\n",
    "# # 5. 모델 평가하기\n",
    "# score, acc = model.evaluate(k_vec_test, k_y_test, batch_size=batch_size)\n",
    "# # score, acc = model.evaluate(k_vec_test, k_y_test,batch_size=None)\n",
    "# print('')\n",
    "# print('score : ' + str(score)+ 'acc : ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in intermediate_output:\n",
    "#     print(i)\n",
    "# type(intermediate_output[0][0])\n",
    "type(k_y_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 binary값\n",
      "[ 3.56274462  3.54656816  3.17959499  4.14543724  3.65040946  3.40665507\n",
      "  3.39636278  3.87953687  3.49503088  3.04400015  3.73767209]\n",
      "1번째 binary값\n",
      "[ 3.50357461  3.29399228  3.0648303   4.22947741  3.22625685  3.60017586\n",
      "  3.347965    4.01286411  3.58111811  3.00994802  3.0973928 ]\n",
      "2번째 binary값\n",
      "[ 3.32466125  3.50519013  3.46845579  3.96044707  3.30048609  3.43269682\n",
      "  3.53478909  3.61211395  3.6979661   3.05185151  3.31565237]\n",
      "3번째 binary값\n",
      "[ 3.46939039  3.39872599  3.23184872  4.1199913   3.17656398  3.33269644\n",
      "  3.44089484  3.93464756  3.58289242  3.04069376  3.18482375]\n",
      "4번째 binary값\n",
      "[ 3.39366627  3.41134953  3.70755839  4.1231122   3.77722955  3.37591076\n",
      "  3.52570701  3.92815566  3.57129025  2.95168853  3.14350152]\n",
      "5번째 binary값\n",
      "[ 3.57131648  3.40438318  3.17216611  4.07928133  3.20399594  3.32768655\n",
      "  3.49779844  3.81893468  3.56745791  3.04154682  3.18881297]\n",
      "6번째 binary값\n",
      "[ 3.45941973  3.62555122  3.06791711  4.17387772  3.69996285  3.35238695\n",
      "  3.55867124  2.91193724  3.58656788  3.03430319  3.09785223]\n",
      "7번째 binary값\n",
      "[ 3.31391335  3.40566564  3.5277977   3.97763824  3.64762712  3.38914037\n",
      "  3.42098784  3.6402514   3.67663693  2.99256063  3.23457885]\n",
      "8번째 binary값\n",
      "[ 3.50175047  3.31266761  3.82532024  4.23923683  3.73951817  3.31279707\n",
      "  3.56138253  4.070539    3.36683774  2.97866416  3.08639383]\n",
      "9번째 binary값\n",
      "[ 3.40867853  3.40002275  3.65890527  4.06279325  3.661484    3.37242222\n",
      "  3.52057648  3.88306165  3.54976296  3.07567787  3.21388912]\n",
      "10번째 binary값\n",
      "[ 3.53226542  3.36054039  3.72215891  4.07365894  3.68323326  3.40222549\n",
      "  3.54715824  3.04131246  3.38333201  3.05067873  3.17145491]\n",
      "11번째 binary값\n",
      "[ 3.50042772  3.37626791  3.18741536  4.10303211  3.63754463  3.3518815\n",
      "  3.44816995  3.86034799  3.58375525  3.05308414  3.19584012]\n",
      "12번째 binary값\n",
      "[ 3.46865058  3.31240487  3.16803336  4.20053911  3.16587019  3.54703116\n",
      "  3.56947875  3.98069572  3.52010798  3.08013439  3.094383  ]\n",
      "13번째 binary값\n",
      "[ 3.29936957  3.54083538  3.54332972  3.89685607  3.66987109  3.45968604\n",
      "  3.32380414  3.54762626  3.23324013  3.09314704  3.35694551]\n",
      "14번째 binary값\n",
      "[ 3.39893746  3.36300159  3.70995164  4.12581539  3.63141489  3.34935856\n",
      "  3.49610686  3.86882687  3.56073189  3.05413365  3.13540792]\n",
      "15번째 binary값\n",
      "[ 3.36827612  3.41429806  3.77506137  4.15349817  3.72703004  3.35099292\n",
      "  3.50422263  3.84334874  3.4222436   3.08304477  3.19712329]\n",
      "16번째 binary값\n",
      "[ 3.49230528  3.28663826  3.9899776   2.57751513  3.79987383  3.23480439\n",
      "  3.60105133  4.14163208  3.50605178  2.98137355  2.9567697 ]\n",
      "17번째 binary값\n",
      "[ 3.49793935  3.28472185  3.92606354  4.29925776  3.16320086  3.29167724\n",
      "  3.61702323  4.19787741  3.40954423  2.97672153  2.93922424]\n",
      "18번째 binary값\n",
      "[ 3.42643046  3.32984352  2.9994185   4.27811527  3.15303016  3.27170992\n",
      "  3.28403354  4.12342548  3.49696612  2.98041797  3.00670457]\n",
      "19번째 binary값\n",
      "[ 3.25665712  3.4744091   3.37373924  3.77035379  3.64465189  3.38122797\n",
      "  3.41994476  3.50927973  3.25778389  3.00681305  3.39952731]\n",
      "20번째 binary값\n",
      "[ 3.39587545  3.32675266  3.71517944  4.15528059  3.65631795  3.41007614\n",
      "  3.47539234  3.80870008  3.55134797  3.07041192  3.1344831 ]\n",
      "21번째 binary값\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-b955b164aa3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"번째 binary값\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1277\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops as tf_ops\n",
    "from tensorflow.python.training import moving_averages\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import functional_ops\n",
    "from tensorflow.python.ops import ctc_ops as ctc\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.core.protobuf import config_pb2\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from distutils.version import StrictVersion\n",
    "import os\n",
    "\n",
    "# from .common import floatx\n",
    "# from .common import epsilon\n",
    "# from .common import normalize_data_format\n",
    "# from ..utils.generic_utils import transpose_shape\n",
    "# from ..utils.generic_utils import has_arg\n",
    "\n",
    "# # Legacy functions\n",
    "# from .common import set_image_dim_ordering\n",
    "# from .common import image_dim_ordering\n",
    "# epsilon()\n",
    "\n",
    "def _to_tensor(x, dtype):\n",
    "    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n",
    "    # Arguments\n",
    "        x: An object to be converted (numpy array, list, tensors).\n",
    "        dtype: The destination type.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    return tf.convert_to_tensor(x, dtype=dtype)\n",
    "\n",
    "def binary_crossentropy(target, output, from_logits=False):\n",
    "    \"\"\"Binary crossentropy between an output tensor and a target tensor.\n",
    "    # Arguments\n",
    "        target: A tensor with the same shape as `output`.\n",
    "        output: A tensor.\n",
    "        from_logits: Whether `output` is expected to be a logits tensor.\n",
    "            By default, we consider that `output`\n",
    "            encodes a probability distribution.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    # Note: tf.nn.sigmoid_cross_entropy_with_logits\n",
    "    # expects logits, Keras expects probabilities.\n",
    "    if not from_logits:\n",
    "        # transform back to logits\n",
    "        _epsilon = _to_tensor(1e-3, 'float32')\n",
    "        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "        output = tf.log(output / (1 - output))\n",
    "\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n",
    "                                                   logits=output)\n",
    "sess = tf.Session()\n",
    "for i in range(len(intermediate_output)):\n",
    "    print(str(i)+\"번째 binary값\")\n",
    "    print(sess.run(binary_crossentropy(intermediate_output[i],k_y_train[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 loss값\n",
      "0.770278\n",
      "1번째 loss값\n",
      "0.801499\n",
      "2번째 loss값\n",
      "0.744164\n",
      "3번째 loss값\n",
      "0.869105\n",
      "4번째 loss값\n",
      "0.851815\n",
      "5번째 loss값\n",
      "0.924549\n",
      "6번째 loss값\n",
      "0.84073\n",
      "7번째 loss값\n",
      "0.841391\n",
      "8번째 loss값\n",
      "0.85332\n",
      "9번째 loss값\n",
      "0.849299\n",
      "10번째 loss값\n",
      "0.797735\n",
      "11번째 loss값\n",
      "1.0\n",
      "12번째 loss값\n",
      "0.84415\n",
      "13번째 loss값\n",
      "0.853868\n",
      "14번째 loss값\n",
      "0.85255\n",
      "15번째 loss값\n",
      "0.795877\n",
      "16번째 loss값\n",
      "0.795859\n",
      "17번째 loss값\n",
      "0.761163\n",
      "18번째 loss값\n",
      "0.872157\n",
      "19번째 loss값\n",
      "0.785898\n",
      "20번째 loss값\n",
      "0.850252\n",
      "21번째 loss값\n",
      "0.852842\n",
      "22번째 loss값\n",
      "0.802451\n",
      "23번째 loss값\n",
      "0.843368\n",
      "24번째 loss값\n",
      "0.74084\n",
      "25번째 loss값\n",
      "0.862723\n",
      "26번째 loss값\n",
      "0.874518\n",
      "27번째 loss값\n",
      "0.79732\n",
      "28번째 loss값\n",
      "0.745591\n",
      "29번째 loss값\n",
      "0.856394\n",
      "30번째 loss값\n",
      "0.740749\n",
      "31번째 loss값\n",
      "0.779374\n",
      "32번째 loss값\n",
      "0.797959\n",
      "33번째 loss값\n",
      "0.781422\n",
      "34번째 loss값\n",
      "0.803129\n",
      "35번째 loss값\n",
      "0.839571\n",
      "36번째 loss값\n",
      "0.872567\n",
      "37번째 loss값\n",
      "0.928037\n",
      "38번째 loss값\n",
      "1.0\n",
      "39번째 loss값\n",
      "0.722814\n",
      "40번째 loss값\n",
      "0.755334\n",
      "41번째 loss값\n",
      "0.863876\n",
      "42번째 loss값\n",
      "0.914002\n",
      "43번째 loss값\n",
      "0.840422\n",
      "44번째 loss값\n",
      "0.744093\n",
      "45번째 loss값\n",
      "0.791272\n",
      "46번째 loss값\n",
      "0.804088\n",
      "47번째 loss값\n",
      "0.790111\n",
      "48번째 loss값\n",
      "0.774096\n",
      "49번째 loss값\n",
      "1.0\n",
      "50번째 loss값\n",
      "0.79464\n",
      "51번째 loss값\n",
      "0.750496\n",
      "52번째 loss값\n",
      "0.925576\n",
      "53번째 loss값\n",
      "0.80329\n",
      "54번째 loss값\n",
      "0.845973\n",
      "55번째 loss값\n",
      "0.840335\n",
      "56번째 loss값\n",
      "0.860204\n",
      "57번째 loss값\n",
      "0.772752\n",
      "58번째 loss값\n",
      "0.844249\n",
      "59번째 loss값\n",
      "0.839181\n",
      "60번째 loss값\n",
      "0.738062\n",
      "61번째 loss값\n",
      "0.725331\n",
      "62번째 loss값\n",
      "0.843515\n",
      "63번째 loss값\n",
      "0.86707\n",
      "64번째 loss값\n",
      "0.869548\n",
      "65번째 loss값\n",
      "0.924259\n",
      "66번째 loss값\n",
      "0.864788\n",
      "67번째 loss값\n",
      "0.845759\n",
      "68번째 loss값\n",
      "0.747908\n",
      "69번째 loss값\n",
      "0.912324\n",
      "70번째 loss값\n",
      "0.830529\n",
      "71번째 loss값\n",
      "0.855009\n",
      "72번째 loss값\n",
      "0.802623\n",
      "73번째 loss값\n",
      "0.801233\n",
      "74번째 loss값\n",
      "nan\n",
      "75번째 loss값\n",
      "0.852872\n",
      "76번째 loss값\n",
      "0.81148\n",
      "77번째 loss값\n",
      "0.795716\n",
      "78번째 loss값\n",
      "1.0\n",
      "79번째 loss값\n",
      "0.785414\n",
      "80번째 loss값\n",
      "0.925348\n",
      "81번째 loss값\n",
      "0.687172\n",
      "82번째 loss값\n",
      "0.865883\n",
      "83번째 loss값\n",
      "0.861473\n",
      "84번째 loss값\n",
      "0.92495\n",
      "85번째 loss값\n",
      "0.852682\n",
      "86번째 loss값\n",
      "0.868076\n",
      "87번째 loss값\n",
      "0.799987\n",
      "88번째 loss값\n",
      "0.84632\n",
      "89번째 loss값\n",
      "0.797577\n",
      "90번째 loss값\n",
      "0.739307\n",
      "91번째 loss값\n",
      "0.860652\n",
      "92번째 loss값\n",
      "0.873103\n",
      "93번째 loss값\n",
      "0.788003\n",
      "94번째 loss값\n",
      "0.79044\n",
      "95번째 loss값\n",
      "0.852262\n",
      "96번째 loss값\n",
      "0.922533\n",
      "97번째 loss값\n",
      "0.874145\n",
      "98번째 loss값\n",
      "0.91472\n",
      "99번째 loss값\n",
      "0.924658\n",
      "100번째 loss값\n",
      "0.775866\n",
      "101번째 loss값\n",
      "0.870993\n",
      "102번째 loss값\n",
      "0.847113\n",
      "103번째 loss값\n",
      "0.801812\n",
      "104번째 loss값\n",
      "0.866737\n",
      "105번째 loss값\n",
      "0.864651\n",
      "106번째 loss값\n",
      "0.860334\n",
      "107번째 loss값\n",
      "0.78505\n",
      "108번째 loss값\n",
      "0.837146\n",
      "109번째 loss값\n",
      "0.798863\n",
      "110번째 loss값\n",
      "0.918388\n",
      "111번째 loss값\n",
      "0.848835\n",
      "112번째 loss값\n",
      "1.0\n",
      "113번째 loss값\n",
      "0.847734\n",
      "114번째 loss값\n",
      "0.742442\n",
      "115번째 loss값\n",
      "0.925918\n",
      "116번째 loss값\n",
      "0.797841\n",
      "117번째 loss값\n",
      "0.78231\n",
      "118번째 loss값\n",
      "0.804804\n",
      "119번째 loss값\n",
      "0.87348\n",
      "120번째 loss값\n",
      "0.655081\n",
      "121번째 loss값\n",
      "0.84564\n",
      "122번째 loss값\n",
      "0.796265\n",
      "123번째 loss값\n",
      "0.758929\n",
      "124번째 loss값\n",
      "0.900371\n",
      "125번째 loss값\n",
      "0.840001\n",
      "126번째 loss값\n",
      "0.745748\n",
      "127번째 loss값\n",
      "0.923991\n",
      "128번째 loss값\n",
      "0.862798\n",
      "129번째 loss값\n",
      "nan\n",
      "130번째 loss값\n",
      "0.846588\n",
      "131번째 loss값\n",
      "0.924985\n",
      "132번째 loss값\n",
      "0.914643\n",
      "133번째 loss값\n",
      "0.786739\n",
      "134번째 loss값\n",
      "0.85978\n",
      "135번째 loss값\n",
      "0.751769\n",
      "136번째 loss값\n",
      "0.805811\n",
      "137번째 loss값\n",
      "0.935423\n",
      "138번째 loss값\n",
      "0.812592\n",
      "139번째 loss값\n",
      "0.853054\n",
      "140번째 loss값\n",
      "0.739103\n",
      "141번째 loss값\n",
      "0.841834\n",
      "142번째 loss값\n",
      "0.910432\n",
      "143번째 loss값\n",
      "0.753527\n",
      "144번째 loss값\n",
      "0.903445\n",
      "145번째 loss값\n",
      "0.871953\n",
      "146번째 loss값\n",
      "0.924525\n",
      "147번째 loss값\n",
      "0.849519\n",
      "148번째 loss값\n",
      "0.755444\n",
      "149번째 loss값\n",
      "nan\n",
      "150번째 loss값\n",
      "0.860468\n",
      "151번째 loss값\n",
      "0.924802\n",
      "152번째 loss값\n",
      "0.751325\n",
      "153번째 loss값\n",
      "0.777527\n",
      "154번째 loss값\n",
      "0.802286\n",
      "155번째 loss값\n",
      "0.785418\n",
      "156번째 loss값\n",
      "nan\n",
      "157번째 loss값\n",
      "0.728364\n",
      "158번째 loss값\n",
      "0.849826\n",
      "159번째 loss값\n",
      "0.786726\n",
      "160번째 loss값\n",
      "0.841982\n",
      "161번째 loss값\n",
      "0.792945\n",
      "162번째 loss값\n",
      "0.927228\n",
      "163번째 loss값\n",
      "0.755855\n",
      "164번째 loss값\n",
      "0.918134\n",
      "165번째 loss값\n",
      "0.793206\n",
      "166번째 loss값\n",
      "0.845129\n",
      "167번째 loss값\n",
      "0.785767\n",
      "168번째 loss값\n",
      "0.804155\n",
      "169번째 loss값\n",
      "0.911926\n",
      "170번째 loss값\n",
      "0.790248\n",
      "171번째 loss값\n",
      "0.910002\n",
      "172번째 loss값\n",
      "0.847597\n",
      "173번째 loss값\n",
      "0.789052\n",
      "174번째 loss값\n",
      "0.730146\n",
      "175번째 loss값\n",
      "0.923484\n",
      "176번째 loss값\n",
      "0.755769\n",
      "177번째 loss값\n",
      "0.850401\n",
      "178번째 loss값\n",
      "0.869448\n",
      "179번째 loss값\n",
      "0.84205\n",
      "180번째 loss값\n",
      "0.789285\n",
      "181번째 loss값\n",
      "0.803171\n",
      "182번째 loss값\n",
      "0.848485\n",
      "183번째 loss값\n",
      "0.807979\n",
      "184번째 loss값\n",
      "0.854174\n",
      "185번째 loss값\n",
      "nan\n",
      "186번째 loss값\n",
      "0.843059\n",
      "187번째 loss값\n",
      "0.79471\n",
      "188번째 loss값\n",
      "0.869724\n",
      "189번째 loss값\n",
      "0.921691\n",
      "190번째 loss값\n",
      "0.798041\n",
      "191번째 loss값\n",
      "0.764445\n",
      "192번째 loss값\n",
      "0.85438\n",
      "193번째 loss값\n",
      "0.847955\n",
      "194번째 loss값\n",
      "0.843309\n",
      "195번째 loss값\n",
      "0.858126\n",
      "196번째 loss값\n",
      "0.857812\n",
      "197번째 loss값\n",
      "0.848417\n",
      "198번째 loss값\n",
      "0.781872\n",
      "199번째 loss값\n",
      "0.840367\n",
      "200번째 loss값\n",
      "0.85933\n",
      "201번째 loss값\n",
      "0.798725\n",
      "202번째 loss값\n",
      "0.859359\n",
      "203번째 loss값\n",
      "0.842562\n",
      "204번째 loss값\n",
      "0.833948\n",
      "205번째 loss값\n",
      "0.804933\n",
      "206번째 loss값\n",
      "0.79193\n",
      "207번째 loss값\n",
      "0.848267\n",
      "208번째 loss값\n",
      "0.848878\n",
      "209번째 loss값\n",
      "0.763063\n",
      "210번째 loss값\n",
      "0.73928\n",
      "211번째 loss값\n",
      "0.783732\n",
      "212번째 loss값\n",
      "0.828234\n",
      "213번째 loss값\n",
      "0.799082\n",
      "214번째 loss값\n",
      "0.785164\n",
      "215번째 loss값\n",
      "0.834632\n",
      "216번째 loss값\n",
      "0.746534\n",
      "217번째 loss값\n",
      "0.920394\n",
      "218번째 loss값\n",
      "0.848421\n",
      "219번째 loss값\n",
      "0.859907\n",
      "220번째 loss값\n",
      "0.846538\n",
      "221번째 loss값\n",
      "0.866477\n",
      "222번째 loss값\n",
      "0.926023\n",
      "223번째 loss값\n",
      "0.789435\n",
      "224번째 loss값\n",
      "0.768878\n",
      "225번째 loss값\n",
      "0.839689\n",
      "226번째 loss값\n",
      "0.863832\n",
      "227번째 loss값\n",
      "0.868622\n",
      "228번째 loss값\n",
      "0.730367\n",
      "229번째 loss값\n",
      "0.798153\n",
      "230번째 loss값\n",
      "0.848258\n",
      "231번째 loss값\n",
      "0.921507\n",
      "232번째 loss값\n",
      "0.734541\n",
      "233번째 loss값\n",
      "0.854841\n",
      "234번째 loss값\n",
      "0.768103\n",
      "235번째 loss값\n",
      "1.0\n",
      "236번째 loss값\n",
      "0.80119\n",
      "237번째 loss값\n",
      "0.796256\n",
      "238번째 loss값\n",
      "0.899963\n",
      "239번째 loss값\n",
      "0.79597\n",
      "240번째 loss값\n",
      "0.849375\n",
      "241번째 loss값\n",
      "0.790301\n",
      "242번째 loss값\n",
      "0.800301\n",
      "243번째 loss값\n",
      "1.0\n",
      "244번째 loss값\n",
      "0.858122\n",
      "245번째 loss값\n",
      "0.79215\n",
      "246번째 loss값\n",
      "0.780055\n",
      "247번째 loss값\n",
      "0.875167\n",
      "248번째 loss값\n",
      "1.0\n",
      "249번째 loss값\n",
      "0.773463\n",
      "250번째 loss값\n",
      "0.777478\n",
      "251번째 loss값\n",
      "0.799023\n",
      "252번째 loss값\n",
      "0.924621\n",
      "253번째 loss값\n",
      "0.812484\n",
      "254번째 loss값\n",
      "0.844226\n",
      "255번째 loss값\n",
      "0.910778\n",
      "256번째 loss값\n",
      "0.922014\n",
      "257번째 loss값\n",
      "0.930529\n",
      "258번째 loss값\n",
      "0.867845\n",
      "259번째 loss값\n",
      "0.712668\n",
      "260번째 loss값\n",
      "0.903495\n",
      "261번째 loss값\n",
      "0.857579\n",
      "262번째 loss값\n",
      "0.844416\n",
      "263번째 loss값\n",
      "0.70628\n",
      "264번째 loss값\n",
      "0.861383\n",
      "265번째 loss값\n",
      "0.797741\n",
      "266번째 loss값\n",
      "0.797198\n",
      "267번째 loss값\n",
      "0.779111\n",
      "268번째 loss값\n",
      "0.799923\n",
      "269번째 loss값\n",
      "0.849728\n",
      "270번째 loss값\n",
      "0.749279\n",
      "271번째 loss값\n",
      "0.843221\n",
      "272번째 loss값\n",
      "0.858815\n",
      "273번째 loss값\n",
      "0.865326\n",
      "274번째 loss값\n",
      "0.778252\n",
      "275번째 loss값\n",
      "0.8038\n",
      "276번째 loss값\n",
      "0.91206\n",
      "277번째 loss값\n",
      "0.923651\n",
      "278번째 loss값\n",
      "0.766362\n",
      "279번째 loss값\n",
      "0.923509\n",
      "280번째 loss값\n",
      "0.924103\n",
      "281번째 loss값\n",
      "0.797316\n",
      "282번째 loss값\n",
      "0.757977\n",
      "283번째 loss값\n",
      "0.868548\n",
      "284번째 loss값\n",
      "0.923437\n",
      "285번째 loss값\n",
      "0.675996\n",
      "286번째 loss값\n",
      "0.777798\n",
      "287번째 loss값\n",
      "0.801976\n",
      "288번째 loss값\n",
      "0.908355\n",
      "289번째 loss값\n",
      "0.713278\n",
      "290번째 loss값\n",
      "0.800218\n",
      "291번째 loss값\n",
      "0.924593\n",
      "292번째 loss값\n",
      "0.690643\n",
      "293번째 loss값\n",
      "0.790573\n",
      "294번째 loss값\n",
      "0.855815\n",
      "295번째 loss값\n",
      "0.786984\n",
      "296번째 loss값\n",
      "0.909064\n",
      "297번째 loss값\n",
      "0.925567\n",
      "298번째 loss값\n",
      "0.790656\n",
      "299번째 loss값\n",
      "0.910076\n",
      "300번째 loss값\n",
      "0.923783\n",
      "301번째 loss값\n",
      "0.928977\n",
      "302번째 loss값\n",
      "0.796417\n",
      "303번째 loss값\n",
      "0.812584\n",
      "304번째 loss값\n",
      "0.842938\n",
      "305번째 loss값\n",
      "0.924009\n",
      "306번째 loss값\n",
      "0.865297\n",
      "307번째 loss값\n",
      "0.848386\n",
      "308번째 loss값\n",
      "0.925457\n",
      "309번째 loss값\n",
      "0.868497\n",
      "310번째 loss값\n",
      "0.926315\n",
      "311번째 loss값\n",
      "0.731428\n",
      "312번째 loss값\n",
      "0.901963\n",
      "313번째 loss값\n",
      "0.851766\n",
      "314번째 loss값\n",
      "0.785882\n",
      "315번째 loss값\n",
      "0.794556\n",
      "316번째 loss값\n",
      "0.736811\n",
      "317번째 loss값\n",
      "0.788252\n",
      "318번째 loss값\n",
      "0.647942\n",
      "319번째 loss값\n",
      "0.801225\n",
      "320번째 loss값\n",
      "0.850145\n",
      "321번째 loss값\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-fe44df2e4715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard_distance_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"번째 loss값\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1277\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(type(intermediate_output[0][0]))\n",
    "# print(k_y_train[0])\n",
    "# print(intermediate_output[0])\n",
    "sess = tf.Session()\n",
    "for i in range(len(intermediate_output)):\n",
    "    a = jaccard_distance_loss(k_y_train[i],intermediate_output[i])\n",
    "    print(str(i)+\"번째 loss값\")\n",
    "    print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.45663142  0.55807221  0.50074631  0.52394301  0.44589725  0.53704381\n",
      "  0.39955401  0.45307899  0.45444661  0.43341556  0.60899854]\n",
      "[  0.   0.   0.   0.   0.   0.   1.   1.   0.   0.  nan]\n"
     ]
    }
   ],
   "source": [
    "print(intermediate_output[185])\n",
    "print(k_y_train[185])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.convert_to_tensor(np.array([0.50548059,0.50548059,0,0,0,0,1,0,0,0,1]), dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79676378"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = jaccard_distance_loss(k_y_train[0],intermediate_output[0])\n",
    "sess = tf.Session()\n",
    "sess.run(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'y' of 'Add' Op has type float64 that does not match type float32 of argument 'x'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    982\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype float32 for Tensor with dtype float64: 'Tensor(\"Abs_20:0\", shape=(11,), dtype=float64)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e036d407097b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard_distance_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-2ec7faeac576>\u001b[0m in \u001b[0;36mjaccard_distance_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0msum_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mintersection\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 301\u001b[0;31m         \"Add\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    544\u001b[0m                   \u001b[0;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[0;32m--> 546\u001b[0;31m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'y' of 'Add' Op has type float64 that does not match type float32 of argument 'x'."
     ]
    }
   ],
   "source": [
    "b = jaccard_distance_loss(k_y_train[0],test)\n",
    "sess = tf.Session()\n",
    "sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-92b751f696a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     print(i[0],y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#             final_x.append([1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "final_x = list()\n",
    "final_y = list()\n",
    "k = [[1,2,3,4,'nan'],[1,2,7,4,5],[1,2,3,4,5]]\n",
    "for i,y in  np.ndenumerate(np.array(k)):\n",
    "#     print(i[0],y)\n",
    "    for j in y:\n",
    "        if (np.isnan(j)==True):\n",
    "            print(y)\n",
    "#             final_x.append([1])\n",
    "#             final_y.append([2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "numk=np.array(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True]\n",
      " [ True  True  True  True]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[np.NaN,np.NaN,np.NaN,np.NaN],[np.NaN,np.NaN,np.NaN,np.NaN]])\n",
    "print(np.isnan(a))\n",
    "print(np.any(np.isnan(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(k_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.any(np.isnan(k_vec)))\n",
    "print(np.any(np.isnan(k_vec_val)))\n",
    "print(np.any(np.isnan(k_vec_test)))\n",
    "print(np.any(np.isnan(k_y_train)))\n",
    "print(np.any(np.isnan(k_y_val)))\n",
    "print(np.any(np.isnan(k_y_test)))\n",
    "print(np.any(np.isnan(k_embedding_matrix)))\n",
    "k_vec = word_vec\n",
    "k_vec_val = word_vec_val\n",
    "k_vec_test = word_vec_test\n",
    "# k_y_train = y_train\n",
    "# k_y_val = y_val\n",
    "# k_y_test = y_test\n",
    "# k_embedding_matrix = embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "ttt = k_y_train[~np.isnan(k_y_train).any(axis=1)]\n",
    "print(np.any(np.isnan(ttt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63  10]\n",
      " [112  10]\n",
      " [189  10]\n",
      " [266  10]\n",
      " [301  10]\n",
      " [350  10]\n",
      " [390  10]\n",
      " [477  10]\n",
      " [584  10]\n",
      " [585  10]\n",
      " [624  10]\n",
      " [663   0]\n",
      " [687  10]\n",
      " [744  10]\n",
      " [773  10]\n",
      " [848  10]]\n"
     ]
    }
   ],
   "source": [
    "index = np.argwhere(np.isnan(k_y_val))[:][0]\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[np.nan,5,2,3],[1,2,3,4],\n",
    "              [2,3,np.nan,5],\n",
    "              [np.nan,5,2,3],[np.nan,5,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argwhere(np.isnan(k_y_train))[:,0]\n",
    "index2 = np.argwhere(np.isnan(k_y_val))[:,0]\n",
    "index3 = np.argwhere(np.isnan(k_y_test))[:,0]\n",
    "\n",
    "k_y_train = np.delete(k_y_train,index,0)\n",
    "k_vec = np.delete(k_vec,index,0)\n",
    "k_y_val = np.delete(k_y_val,index2,0)\n",
    "k_vec_val = np.delete(k_vec_val,index2,0)\n",
    "k_vec_test = np.delete(k_vec_test,index3,0)\n",
    "k_y_test = np.delete(k_y_test,index3,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.any(np.isnan(k_vec)))\n",
    "print(np.any(np.isnan(k_vec_val)))\n",
    "print(np.any(np.isnan(k_vec_test)))\n",
    "print(np.any(np.isnan(k_y_train)))\n",
    "print(np.any(np.isnan(k_y_val)))\n",
    "print(np.any(np.isnan(k_y_test)))\n",
    "print(np.any(np.isnan(k_embedding_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 2 4]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1,1,1,1],[2,2,2,4],[3,3,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print(np.delete(arr,index,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
