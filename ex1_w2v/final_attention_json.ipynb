{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_array(6651, 13)\n",
      "val_array(887, 13)\n",
      "test_array(3239, 13)\n",
      "Reading data!\n",
      "checking Anomaly!!\n",
      "(6619,)\n",
      "(6619, 11)\n",
      "(883,)\n",
      "(883, 11)\n",
      "(3215,)\n",
      "(3215, 11)\n",
      "ANomaly result!\n",
      "x_train.shape(6619,)\n",
      "y_train.shape(6619, 11)\n",
      "x_val.shape(883,)\n",
      "y_val.shape(883, 11)\n",
      "x_test.shape(3215,)\n",
      "y_test.shape(3215, 11)\n",
      "Finished!\n",
      "Tokenizing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "828it [00:00, 8273.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing finished!\n",
      "It's by len()\n",
      "train_tmp.len():6619,40\n",
      "val_tmp shape.len():883,39\n",
      "test_tmp shape.len():3215,46\n",
      "max_count:10531\n",
      "Readding Embedding file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45942it [00:05, 7826.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45942 word vectors.\n",
      "word_vec shape:6619,100\n",
      "word_vec_val shape:883,100\n",
      "word_vec_test shape:3215,100\n",
      "match:11300\n",
      "unmatch:12626\n",
      "word_vec shape:6619,100\n",
      "word_vec_val shape:883,100\n",
      "word_vec_test shape:3215,100\n",
      "word_vec shape:(6619, 100)\n",
      "word_vec_val shape:(883, 100)\n",
      "word_vec_test shape:(3215, 100)\n",
      "y_train shape:(6619, 11)\n",
      "y_val shape:(883, 11)\n",
      "y_test shape:(3215, 11)\n",
      "embedding_matrix shape:(23927, 300)\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minwookje/coding/ex1_w2v/Attention.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(500, kernel_initializer=\"glorot_uniform\", activation=\"tanh\", name=\"tanh_mlp\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg...)`\n",
      "  x_a = Dense(repeat_size, kernel_initializer = 'glorot_uniform', activation=\"tanh\",name=\"tanh_mlp\",W_regularizer=W_reg,b_regularizer=b_reg)(inp)\n",
      "/home/minwookje/coding/ex1_w2v/Attention.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(500, kernel_initializer=\"glorot_uniform\", activation=\"tanh\", name=\"tanh_mlp2\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg...)`\n",
      "  x_a = Dense(repeat_size, kernel_initializer = 'glorot_uniform', activation=\"tanh\", name=\"tanh_mlp2\",W_regularizer=W_reg,b_regularizer=b_reg)(x_a)\n",
      "/home/minwookje/coding/ex1_w2v/Attention.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"linear\", name=\"word-level_context\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg...)`\n",
      "  x_a = Dense(1, kernel_initializer = 'glorot_uniform', activation='linear', name=\"word-level_context\",W_regularizer=W_reg,b_regularizer=b_reg)(x_a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     7178100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 100, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 500)     1102000     spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tanh_mlp (Dense)                (None, 100, 500)     250500      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 500)     0           tanh_mlp[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tanh_mlp2 (Dense)               (None, 100, 500)     250500      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100, 500)     0           tanh_mlp2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word-level_context (Dense)      (None, 100, 1)       501         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100, 1)       0           word-level_context[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 500, 100)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 100, 500)     0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 100, 500)     0           bidirectional_1[0][0]            \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "expectation_over_words (Lambda) (None, 500)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 11)           5511        expectation_over_words[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 8,787,112\n",
      "Trainable params: 1,609,012\n",
      "Non-trainable params: 7,178,100\n",
      "__________________________________________________________________________________________________\n",
      "Train on 6474 samples, validate on 867 samples\n",
      "Epoch 1/16\n",
      "6474/6474 [==============================] - 190s 29ms/step - loss: 0.5608 - jaccard_distance_acc: 0.1626 - fbeta: 0.0470 - val_loss: 0.5009 - val_jaccard_distance_acc: 0.1787 - val_fbeta: 0.0161\n",
      "Epoch 2/16\n",
      "6474/6474 [==============================] - 174s 27ms/step - loss: 0.4857 - jaccard_distance_acc: 0.1758 - fbeta: 0.1029 - val_loss: 0.4700 - val_jaccard_distance_acc: 0.2001 - val_fbeta: 0.1371\n",
      "Epoch 3/16\n",
      "6474/6474 [==============================] - 188s 29ms/step - loss: 0.4689 - jaccard_distance_acc: 0.1960 - fbeta: 0.1936 - val_loss: 0.4627 - val_jaccard_distance_acc: 0.2118 - val_fbeta: 0.2274\n",
      "Epoch 4/16\n",
      "6474/6474 [==============================] - 166s 26ms/step - loss: 0.4602 - jaccard_distance_acc: 0.2087 - fbeta: 0.2523 - val_loss: 0.4490 - val_jaccard_distance_acc: 0.2257 - val_fbeta: 0.2880\n",
      "Epoch 5/16\n",
      "6474/6474 [==============================] - 164s 25ms/step - loss: 0.4490 - jaccard_distance_acc: 0.2208 - fbeta: 0.2853 - val_loss: 0.4434 - val_jaccard_distance_acc: 0.2205 - val_fbeta: 0.2275\n",
      "Epoch 6/16\n",
      "6474/6474 [==============================] - 163s 25ms/step - loss: 0.4433 - jaccard_distance_acc: 0.2273 - fbeta: 0.2996 - val_loss: 0.4407 - val_jaccard_distance_acc: 0.2391 - val_fbeta: 0.2953\n",
      "Epoch 7/16\n",
      "6474/6474 [==============================] - 162s 25ms/step - loss: 0.4392 - jaccard_distance_acc: 0.2336 - fbeta: 0.3137 - val_loss: 0.4391 - val_jaccard_distance_acc: 0.2436 - val_fbeta: 0.3045\n",
      "Epoch 8/16\n",
      "6474/6474 [==============================] - 285s 44ms/step - loss: 0.4327 - jaccard_distance_acc: 0.2419 - fbeta: 0.3322 - val_loss: 0.4355 - val_jaccard_distance_acc: 0.2392 - val_fbeta: 0.2913\n",
      "Epoch 9/16\n",
      "6474/6474 [==============================] - 163s 25ms/step - loss: 0.4283 - jaccard_distance_acc: 0.2490 - fbeta: 0.3508 - val_loss: 0.4339 - val_jaccard_distance_acc: 0.2627 - val_fbeta: 0.3664\n",
      "Epoch 10/16\n",
      "6474/6474 [==============================] - 164s 25ms/step - loss: 0.4250 - jaccard_distance_acc: 0.2552 - fbeta: 0.3631 - val_loss: 0.4353 - val_jaccard_distance_acc: 0.2596 - val_fbeta: 0.3518\n",
      "Epoch 11/16\n",
      "  64/6474 [..............................] - ETA: 2:45 - loss: 0.4004 - jaccard_distance_acc: 0.2668 - fbeta: 0.3834"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, re, csv, math, codecs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer as t\n",
    "from tqdm import tqdm\n",
    "from keras.layers import merge, Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import *\n",
    "# to visualize, and to make zero shape matrix\n",
    "from attention_utils import get_activations, get_data_recurrent\n",
    "from Attention import Attention\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt as Twitter\n",
    "from selfword2vec import tokenization\n",
    "from Anomaly import checkAnomaly_x_y\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from numpy import argmax\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.losses import binary_crossentropy\n",
    "import json\n",
    "# 1. 모델 저장시키기 \n",
    "# 2. tokenizer와 konlpy morphs 호환 여부 (완성 jupyter note)\n",
    "# 2'. word2vec 2가지 경우 더 추가 to embedding(final_total word2vec, twitter_translated.vec)\n",
    "# 3. 변수들 설정하기\n",
    "\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# NUM_WORDS, train으로 input받은 단어의 수\n",
    "MAX_NB_WORDS = 20000\n",
    "vocab_size = 0\n",
    "EMB_DIM = 300\n",
    "embeddings_index = dict()\n",
    "\n",
    "# columns = [\"ID\", \"Tweet\", \"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\",\"sadness\",\"surprise\",\"trust\"]\n",
    "columns = [\"ID\",\"Tweet\",\"분노\",\"기대\",\"혐오스러운\",\"두려움\",\"기쁨\",\"사랑\",\"낙관론\",\"비관론\",\"슬픔\",\"놀라움\",\"믿음\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################1. 데이터셋 생성하기\n",
    "\n",
    "train_array = pd.read_csv(\"/home/minwookje/coding/ex1_w2v/data/tweet/dump/kor_train.txt\",sep=\"\\t\", header=None,names=columns).values\n",
    "val_array = pd.read_csv(\"/home/minwookje/coding/ex1_w2v/data/tweet/dump/kor_dev.txt\",sep=\"\\t\", header=None,names=columns).values\n",
    "test_array = pd.read_csv(\"/home/minwookje/coding/ex1_w2v/data/tweet/dump/kor_test_gold.txt\",sep=\"\\t\", header=None,names=columns).values\n",
    "\n",
    "# 판다 shape\n",
    "print(\"train_array\"+ str(train_array.shape))\n",
    "print(\"val_array\"+str(val_array.shape))\n",
    "print(\"test_array\"+str(test_array.shape))\n",
    "\n",
    "print(\"Reading data!\")\n",
    "#  x, y 분할하기\n",
    "x_train = train_array[1:,1]\n",
    "y_train = train_array[1:,2:]\n",
    "x_val = val_array[1:,1]\n",
    "y_val = val_array[1:,2:]\n",
    "x_test = test_array[1:,1]\n",
    "y_test = test_array[1:,2:]\n",
    "\n",
    "\n",
    "print(\"checking Anomaly!!\")\n",
    "x_train, y_train = checkAnomaly_x_y(x_train,y_train)\n",
    "x_val, y_val = checkAnomaly_x_y(x_val,y_val)\n",
    "x_test, y_test = checkAnomaly_x_y(x_test,y_test)\n",
    "\n",
    "print(\"ANomaly result!\")\n",
    "print(\"x_train.shape\"+ str(x_train.shape))\n",
    "print(\"y_train.shape\"+ str(y_train.shape))\n",
    "print(\"x_val.shape\"+ str(x_val.shape))\n",
    "print(\"y_val.shape\"+ str(y_val.shape))\n",
    "print(\"x_test.shape\"+ str(x_test.shape))\n",
    "print(\"y_test.shape\"+ str(y_test.shape))\n",
    "# print(type(x_train)) np.array로 변형이 필요한가\n",
    "print(\"Finished!\")\n",
    "\n",
    "# jupyter notebook\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "tmp = []\n",
    "train_tmp = []\n",
    "val_tmp = []\n",
    "test_tmp = []\n",
    "max_count = 0\n",
    "set_words = set()\n",
    "\n",
    "print(\"Tokenizing!\")\n",
    "# test의 tmp만 따로 받아주고 나머지는 val train test모두 통합시켜준다.\n",
    "test_tmp, dummy , set_words = tokenization(x_test)\n",
    "val_tmp, dummy , set_words = tokenization(x_val)\n",
    "train_tmp, max_count , set_words = tokenization(x_train)\n",
    "tmp, dummy , set_words = tokenization(np.hstack([x_train,x_val,x_test]))\n",
    "print(\"Tokenizing finished!\")\n",
    "tmp = [] # tmp없애준다\n",
    "\n",
    "\n",
    "\n",
    "#token shape\n",
    "print(\"It's by len()\")\n",
    "print(\"train_tmp.len():\" + str(len(train_tmp))+\",\"+ str(len(train_tmp[0])))\n",
    "print(\"val_tmp shape.len():\" + str(len(val_tmp))+\",\"+ str(len(val_tmp[0])))\n",
    "print(\"test_tmp shape.len():\" + str(len(test_tmp))+\",\"+ str(len(test_tmp[0])))\n",
    "\n",
    "\n",
    "print(\"max_count:\"+str(max_count))\n",
    "# 문장길이 100으로 맞춘다.\n",
    "max_count = min(100, max_count)\n",
    "\n",
    "\n",
    "print(\"Readding Embedding file\")\n",
    "# embeddings_index == dict(w2v's word: vector)\n",
    "f = codecs.open('/home/minwookje/coding/ex1_w2v/embedding/1542954106final_total_pos.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "# word2index table 생성 {token:index}\n",
    "# 모든 train val test에 사용되는 token별 index table\n",
    "# train 문장 token 갯수\n",
    "word_index = len(set_words)\n",
    "word_tmp_dict = dict()\n",
    "for i, word in enumerate(set_words):\n",
    "    word_tmp_dict[str(word).replace(\" \", \"\")] = i\n",
    "word_tmp_dict['0'] = word_index \n",
    "\n",
    "#dict file 저장\n",
    "with open('token2index.json','w') as dictionary_file:\n",
    "    json.dump(word_tmp_dict,dictionary_file)\n",
    "\n",
    "## 문장별 토큰화시킨 녀석에 index를 집어 넣어준다. 이때 pad도 동시에 해준다. \n",
    "word_vec = []\n",
    "word_vec_test = []\n",
    "word_vec_val = []\n",
    "\n",
    "for sent in train_tmp:\n",
    "    sub = []\n",
    "    for word in sent:\n",
    "        if(len(sub)==max_count):\n",
    "            break    \n",
    "         #print(word)\n",
    "         #print(type(str(word)))\n",
    "         #break\n",
    "# word는 tuple 타입, embeddings_index는 str타입, tuple 타입을 str()화시키면 \n",
    "# 중간에 space가 생성되어 match가 되지 않았다. 이를 해결해주었다. \n",
    "        if(str(word).replace(\" \", \"\") in word_tmp_dict):\n",
    "            \n",
    "            sub.append(word_tmp_dict[str(word).replace(\" \", \"\")])\n",
    "        else:\n",
    "            print(\"sentence index화 실패\")\n",
    "    count = max_count - len(sub)\n",
    "    # padding\n",
    "    sub.extend([word_index]*count)\n",
    "    word_vec.append(sub)\n",
    "## 테스트용 복사본 \n",
    "for sent in test_tmp:\n",
    "    sub = []\n",
    "    for word in sent:\n",
    "        if(len(sub)==max_count):\n",
    "            break\n",
    "         #print(word)\n",
    "         #print(type(str(word)))\n",
    "         #break\n",
    "# word는 tuple 타입, embeddings_index는 str타입, tuple 타입을 str()화시키면 \n",
    "# 중간에 space가 생성되어 match가 되지 않았다. 이를 해결해주었다. \n",
    "        if(str(word).replace(\" \", \"\") in word_tmp_dict):\n",
    "            \n",
    "            sub.append(word_tmp_dict[str(word).replace(\" \", \"\")])\n",
    "        else:\n",
    "            print(\"sentence index화 실패\")\n",
    "    count = max_count - len(sub)\n",
    "    # padding\n",
    "    sub.extend([word_index]*count)\n",
    "    word_vec_test.append(sub)\n",
    "\n",
    "## 검증용 복사본 \n",
    "for sent in val_tmp:\n",
    "    sub = []\n",
    "    for word in sent:\n",
    "        if(len(sub)==max_count):\n",
    "            break\n",
    "         #print(word)\n",
    "         #print(type(str(word)))\n",
    "         #break\n",
    "# word는 tuple 타입, embeddings_index는 str타입, tuple 타입을 str()화시키면 \n",
    "# 중간에 space가 생성되어 match가 되지 않았다. 이를 해결해주었다. \n",
    "        if(str(word).replace(\" \", \"\") in word_tmp_dict):\n",
    "            \n",
    "            sub.append(word_tmp_dict[str(word).replace(\" \", \"\")])\n",
    "        else:\n",
    "            print(\"sentence index화 실패\")\n",
    "    count = max_count - len(sub)\n",
    "    # padding\n",
    "    sub.extend([word_index]*count)\n",
    "    word_vec_val.append(sub)\n",
    "\n",
    "print(\"word_vec shape:\" + str(len(word_vec))+\",\"+ str(len(word_vec[0])))\n",
    "print(\"word_vec_val shape:\" + str(len(word_vec_val))+\",\"+ str(len(word_vec_val[0])))\n",
    "print(\"word_vec_test shape:\" + str(len(word_vec_test))+\",\"+ str(len(word_vec_test[0])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4번쨰 matrix embedding_matrix {index: vector}\n",
    "# vocab_size = min(MAX_NB_WORDS, word_index)\n",
    "vocab_size = word_index\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size+1, EMB_DIM))\n",
    "match_count = 0\n",
    "unmatch_count = 0\n",
    "\n",
    "for word, i in word_tmp_dict.items():\n",
    "    if word != '0':\n",
    "        if (word in embeddings_index):\n",
    "            match_count += 1\n",
    "            # embedding_matrix[i] = np.zeros(300)\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "            # embedding_matrix[i] = np.random.uniform(-0.25,0.25,300)\n",
    "        else:\n",
    "            unmatch_count += 1\n",
    "            # embedding_matrix[i] = np.zeros(300)\n",
    "            # embedding_matrix[i] = np.random.uniform(-0.25,0.25,300) ## used for OOV words\n",
    "            embedding_matrix[i] = np.random.uniform(-1.0,1.0,300).astype('float32')\n",
    "print(\"match:\" + str(match_count))\n",
    "print(\"unmatch:\" + str(unmatch_count))\n",
    "\n",
    "embedding_matrix.tofile('index2vec.dat')\n",
    "#     여기부터\n",
    "# 1.앞서 단어당 벡터 테이블(v) // embeddings_index, {w2v_word: vector}\n",
    "# train_word(str(word).replace(\" \", \"\")) == embedding\n",
    "# 2.train 단어별 index (v) // word_tmp_dict {train_word(str(word).replace(\" \", \"\")):index}\n",
    "# 3.sentence padding, sentence to index\n",
    "# 4.index당 vector table (v) //embedding_matrix {index: vector} 이녀석을 embedding weight에 넣어주어야 한다. \n",
    "\n",
    "# 문장 = [index들 나열 ] \n",
    "# 즉 embedding_matrix로 index를 seq에 넣어준묹장들을 train에 넣어줘야한다. \n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# np.array화\n",
    "print(\"word_vec shape:\" + str(len(word_vec))+\",\"+ str(len(word_vec[0])))\n",
    "print(\"word_vec_val shape:\" + str(len(word_vec_val))+\",\"+ str(len(word_vec_val[0])))\n",
    "print(\"word_vec_test shape:\" + str(len(word_vec_test))+\",\"+ str(len(word_vec_test[0])))\n",
    "# np.savetxt('word_vec1.txt', word_vec[:500], delimiter=\" \", fmt=\"%s\") \n",
    "# np.savetxt('word_vec_val1.txt', word_vec_val[:500], delimiter=\" \", fmt=\"%s\") \n",
    "# np.savetxt('word_vec_test1.txt', word_vec_test[:500], delimiter=\" \", fmt=\"%s\") \n",
    "\n",
    "word_vec = np.array(word_vec)\n",
    "word_vec_val = np.array(word_vec_val)\n",
    "word_vec_test = np.array(word_vec_test)\n",
    "embedding_matrix = np.matrix(embedding_matrix)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"word_vec shape:\" + str(word_vec.shape))\n",
    "print(\"word_vec_val shape:\" + str(word_vec_val.shape))\n",
    "print(\"word_vec_test shape:\" + str(word_vec_test.shape))\n",
    "print(\"y_train shape:\" + str(y_train.shape))\n",
    "print(\"y_val shape:\" + str(y_val.shape))\n",
    "print(\"y_test shape:\" + str(y_test.shape))\n",
    "print(\"embedding_matrix shape:\" + str(embedding_matrix.shape))\n",
    "\n",
    "# 확인용\n",
    "np.savetxt('word_vec.txt', word_vec[:500], delimiter=\" \", fmt=\"%s\") \n",
    "np.savetxt('word_vec_val.txt', word_vec_val[:500], delimiter=\" \", fmt=\"%s\") \n",
    "np.savetxt('word_vec_test.txt', word_vec_test[:500], delimiter=\" \", fmt=\"%s\") \n",
    "np.savetxt('embedding_matrix.txt', embedding_matrix[:600], delimiter=\" \", fmt=\"%s\") \n",
    "# TODO\n",
    "# 1.padding 23976이 너무 많이 들어간다. 이거 max_count를 정해주어야 한다. 100까지로 줄이자.\n",
    "# 2.그 다음에는 matrix 사이즈를 맞춰주어야 한다. \n",
    "\n",
    "\n",
    "# def fit_multilabel(model, X_train, X_val, y_train, y_val):\n",
    "#     y_val = np.array(y_val)\n",
    "#     y_train = np.array(y_train)\n",
    "\n",
    "#     predictions = np.zeros(y_val.shape)\n",
    "\n",
    "#     for i in range(y_val.shape[1]):\n",
    "#         model.fit(X_train, y_train[:, i])\n",
    "#         y_p = model.predict(X_val)\n",
    "#         predictions[:, i] = y_p\n",
    "\n",
    "#     return predictions\n",
    "# from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "# def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    # \"\"\"\n",
    "    # Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "    #         = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    # The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    # shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    # gradient.\n",
    "    \n",
    "    # Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    # @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    # @author: wassname\n",
    "    # \"\"\"\n",
    "    # print(\"loss = %s, %s\"%(y_true,y_pred))\n",
    "    # y_pred = K.cast_to_floatx(y_pred)\n",
    "    # y_true = K.cast_to_floatx(y_true)\n",
    "    # intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    # sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    # jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    # print(\"loss = %s, %s, %s\"%(intersection, sum, jac))   \n",
    "    # np.zeros_like(y_true, dtype = object)\n",
    "    # np.zeros_like(y_pred, dtype = object)\n",
    "\n",
    "    # im1 = np.asarray(y_true).astype(np.bool)\n",
    "    # im2 = np.asarray(y_pred).astype(np.bool)\n",
    "    # intersection = np.logical_and(im1, im2)\n",
    "    # union = np.logical_or(im1, im2)\n",
    "    # # intersection = K.sum(np.absolute(y_true * y_pred), axis=-1)\n",
    "    # # sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    # # jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    # jac = (intersection.sum()+smooth) / (float(union.sum()+smooth))\n",
    "    # print(\"loss = %s, %s, %s\"%(intersection, sum, jac))\n",
    "    # print(\"intersection\")\n",
    "    # print(intersection)\n",
    "    # # print(K.eval(intersection))\n",
    "    # print(\"sum\")\n",
    "    # print(sum)\n",
    "    # # print(K.eval(sum))\n",
    "    # print(\"jac\")\n",
    "    # print(jac)\n",
    "    # # print(K.eval(jac))\n",
    "    # print(\"K.eval(1-jac)*smooth\")\n",
    "    # print((1 - jac) * smooth)\n",
    "    # print(type((1 - jac) * smooth))\n",
    "    # print(K.eval(1-jac)*smooth)\n",
    "    # returnimport tensorflow as tf (1 - jac) * smooth\n",
    "\n",
    "# def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "#     intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "#     sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "#     jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "#     return (1 - jac) * smooth\n",
    "#####################################2. 모델 구성하기\n",
    "# input_length : 단어의 수 즉 문장의 길이를 나타냅니다\n",
    "# 임베딩 레이어의 출력 크기는 샘플 수 * output_dim * input_lenth가 됩니다\n",
    "\n",
    "INPUT_DIM = 300 #wordvec사이즈\n",
    "max_count = max_count #문장의 max길이\n",
    "lstm_shape = 250\n",
    "rate_drop_lstm = 0.3\n",
    "rate_drop_dense = 0.3\n",
    "\n",
    "# k_vec = K.variable(word_vec)\n",
    "# k_vec_val = K.variable(word_vec_val)\n",
    "# k_vec_test = K.variable(word_vec_test)\n",
    "# k_y_train = K.variable(y_train)\n",
    "# k_y_val = K.variable(y_val)\n",
    "# k_y_test = K.variable(y_test)\n",
    "# k_embedding_matrix = K.variable(embedding_matrix)\n",
    "k_vec = word_vec\n",
    "k_vec_val = word_vec_val\n",
    "k_vec_test = word_vec_test\n",
    "k_y_train = y_train\n",
    "k_y_val = y_val\n",
    "k_y_test = y_test\n",
    "k_embedding_matrix = embedding_matrix\n",
    "\n",
    "# nan delete\n",
    "index = np.argwhere(np.isnan(k_y_train))[:,0]\n",
    "index2 = np.argwhere(np.isnan(k_y_val))[:,0]\n",
    "index3 = np.argwhere(np.isnan(k_y_test))[:,0]\n",
    "\n",
    "k_y_train = np.delete(k_y_train,index,0)\n",
    "k_vec = np.delete(k_vec,index,0)\n",
    "k_y_val = np.delete(k_y_val,index2,0)\n",
    "k_vec_val = np.delete(k_vec_val,index2,0)\n",
    "k_vec_test = np.delete(k_vec_test,index3,0)\n",
    "k_y_test = np.delete(k_y_test,index3,0)\n",
    "# is nan check\n",
    "print(np.any(np.isnan(k_vec)))\n",
    "print(np.any(np.isnan(k_vec_val)))\n",
    "print(np.any(np.isnan(k_vec_test)))\n",
    "print(np.any(np.isnan(k_y_train)))\n",
    "print(np.any(np.isnan(k_y_val)))\n",
    "print(np.any(np.isnan(k_y_test)))\n",
    "print(np.any(np.isnan(k_embedding_matrix)))\n",
    "\n",
    "\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = tf.convert_to_tensor(1e-7, dtype='float32')\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac)\n",
    "\n",
    "# matrix accu\n",
    "def jaccard_distance_acc(y_true, y_pred):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection) / (sum_ - intersection)\n",
    "    return jac\n",
    "\n",
    "# def scaled_binary_cross_entropy(y_true,y_pred):\n",
    "#     epsilon = tf.convert_to_tensor(1e-7, dtype='float32')\n",
    "#     r =K.binary_crossentropy(y_true,y_pred)\n",
    "    \n",
    "#     return r/(r.max()+epsilon)\n",
    "\n",
    "def scaled_binary_cross_entropy(y_true,y_pred):\n",
    "    epsilon = tf.convert_to_tensor(1e-7, dtype='float32')\n",
    "    loss =binary_crossentropy(y_true,y_pred)\n",
    "    max_t = K.max(loss)\n",
    "    return loss/(max_t+epsilon)\n",
    "\n",
    "\n",
    "def fbeta(y_true, y_pred, threshold_shift=0):\n",
    "    beta = 2\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "    # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin), axis=1) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)), axis=1)\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    beta_squared = beta ** 2\n",
    "    return K.mean((beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "accu = jaccard_distance_acc    \n",
    "\n",
    "\n",
    "# 32하니까 죽어버린다.\n",
    "batch_size = 32\n",
    "l2_reg = 0.0001\n",
    "activity_l2 = 0.0001\n",
    "# Define the model\n",
    "# inp = InputLayer(shape=(max_count,), dtype='float32',sparse=True)\n",
    "# input layer는 \n",
    "# inp = InputLayer(input_shape=(max_count,),sparse=True)\n",
    "\n",
    "# inp = Input(shape=(max_count,))\n",
    "# 생각해보니까 이거 float 필요없다.\n",
    "inp = Input(shape=(max_count,))\n",
    "emb = Embedding(input_dim=vocab_size+1, output_dim=EMB_DIM,\n",
    "            trainable=False, weights=[embedding_matrix], input_length=max_count)(inp)\n",
    "# max_features = vocab_size, maxlen=text_max_words, embed_size=EMB_DIM\n",
    "# emb = Embedding(input_dim=max_features, input_length = maxlen, output_dim=embed_size)(inp)\n",
    "# embedding dropout = 0.1\n",
    "# x = Bidirectional(LSTM(lstm_shape, return_sequences=True, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm, W_regularizer=l2(l2_reg)))(x)\n",
    "# weight, bias, hidden state 수정해보기 , 현재는 kernel만 regularization 시켰다. \n",
    "\n",
    "x = SpatialDropout1D(0.1)(emb)\n",
    "\n",
    "\n",
    "\n",
    "x = Bidirectional(LSTM(lstm_shape, return_sequences=True, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,kernel_regularizer=l2(l2_reg)))(x)\n",
    "x, attention = Attention()(x,rate_drop_dense,l2_reg)\n",
    "\n",
    "\n",
    "# BatchNOrmalization 추가\n",
    "# x = BatchNormalization()(x)\n",
    "\n",
    "# Dense(11, activation=\"sigmoid\",activity_regularizer=activity_l2(0.0001))\n",
    "x = Dense(11, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "# # 1\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#             optimizer='adam',\n",
    "#             metrics=[accu])\n",
    "\n",
    "\n",
    "# 2\n",
    "\n",
    "# model.compile(loss=jaccard_distance_loss,\n",
    "#             optimizer='adam',\n",
    "#             metrics=[accu])\n",
    "\n",
    "# 3\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#             optimizer='adam',\n",
    "#             metrics=['accracy'])\n",
    "\n",
    "# 4\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#             optimizer='rmsprop',\n",
    "#             metrics=['acc', accu])\n",
    "\n",
    "# 5\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#             optimizer=Adam(clipnorm=1, lr=0.001),\n",
    "#             metrics=[accu, fbeta])\n",
    "\n",
    "# 6\n",
    "# model.compile(loss=scaled_binary_cross_entropy,\n",
    "#             optimizer=Adam(clipnorm=1, lr=0.001),\n",
    "#             metrics=[accu, fbeta])\n",
    "\n",
    "# 6' clipnorm=1의 역활이 득이 되는지 아닌지 잘 모르겠다. 평가의 기준이 명확하지 않으니까 \n",
    "# 내가 맞게 평가 하고 학습하는지 잘 모르겠다. \n",
    "# model.compile(loss=scaled_binary_cross_entropy,\n",
    "#             optimizer=Adam(lr=0.001),\n",
    "#             metrics=[accu, fbeta])\n",
    "\n",
    "\n",
    "#7\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=Adam(lr=0.001),\n",
    "            metrics=[accu, fbeta])\n",
    "\n",
    "attention_model = Model(inputs=inp, outputs=attention) # Model to print out the attention data\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# EPOCH 16 적당하다.\n",
    "model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=16, verbose=1, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# model.fit(k_vec, k_y_train, validation_data=(k_vec_val,k_y_val), epochs=3, verbose=1, steps_per_epoch=int(6619/batch_size)+1, validation_steps = int(883/batch_size)+1)\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "test_score = model.evaluate(k_vec_test, k_y_test, batch_size=batch_size)\n",
    "print('')\n",
    "print(str(test_score))\n",
    "\n",
    "\n",
    "\n",
    "# 6. 모델 저장하기\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# 가중치 저장하기\n",
    "model.save_weights('test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
