{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, re, csv, math, codecs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from tqdm import tqdm\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import *\n",
    "# to visualize, and to make zero shape matrix\n",
    "from attention_utils import get_activations, get_data_recurrent\n",
    "# from Attention import attention\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Okt as Twitter\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델 저장시키기\n",
    "# 2. tokenizer와 konlpy morphs 호환 여부\n",
    "# 3. 변수들 설정하기\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# NUM_WORDS, train으로 input받은 단어의 수\n",
    "MAX_NB_WORDS = 20000 \n",
    "EMB_DIM = 300\n",
    "embeddings_index = dict()\n",
    "# INPUT 문장길이\n",
    "text_max_words = 280\n",
    "\n",
    "# columns = [\"ID\", \"Tweet\", \"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\",\"sadness\",\"surprise\",\"trust\"]\n",
    "columns = [\"ID\",\"Tweet\",\"분노\",\"기대\",\"혐오스러운\",\"두려움\",\"기쁨\",\"사랑\",\"낙관론\",\"비관론\",\"슬픔\",\"놀라움\",\"믿음\"]\n",
    "\n",
    "\n",
    "\n",
    "# def parse(dataset, emotion=None):\n",
    "#     with open(data_file, 'r') as fd:\n",
    "#         data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "#     X = [d[1] for d in data]\n",
    "#     # dict.values() does not guarantee the order of the elements\n",
    "#     # so we should avoid using a dict for the labels\n",
    "#     y = [[int(l) for l in d[2:]] for d in data]\n",
    "\n",
    "#     return X, y\n",
    "\n",
    "\n",
    "##########################################1. 데이터셋 생성하기\n",
    "\n",
    "    # X_train, y_train = parse(task='E-c', dataset=\"train\")\n",
    "    # X_dev, y_dev = parse(task='E-c', dataset=\"dev\")\n",
    "    # X_test, y_test = parse(task='E-c', dataset=\"gold\")\n",
    "    # X_train = preprocessor(\"{}_{}\".format(name, \"train\"), X_train)\n",
    "    # X_dev = preprocessor(\"{}_{}\".format(name, \"dev\"), X_dev)\n",
    "    # X_test = preprocessor(\"{}_{}\".format(name, \"test\"), X_test)\n",
    "    # res_dev = fit_multilabel(model, X_train, X_dev, y_train, y_dev)\n",
    "    # res_test = fit_multilabel(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # res_dev = evaluation(res_dev, y_dev)\n",
    "    # res_test = evaluation(res_test, y_test)\n",
    "\n",
    "# train_array = pd.read_csv(\"/home/minwookje/coding/ex1_w2v/data/tweet/dump/kor_train.txt\",sep=\"\\t\", header=None,names=columns).values\n",
    "val_array = pd.read_csv(\"/home/minwookje/coding/ex1_w2v/data/tweet/dump/kor_dev.txt\",sep=\"\\t\", header=None,names=columns).values\n",
    "# test_array = pd.read_csv(\"/home/minwookje/coding/ex1_w2v/data/tweet/dump/kor_test.txt\",sep=\"\\t\", header=None,names=columns).values\n",
    "\n",
    "# print(train_array.shape)\n",
    "# print(val_array.shape)\n",
    "# print(test_array.shape)\n",
    "\n",
    "#  x, y 분할하기\n",
    "# x_train = train_array[:,1]\n",
    "# y_train = train_array[:,2:]\n",
    "x_val = val_array[1:,1]\n",
    "y_val = val_array[1:,2:]\n",
    "# x_test = test_array[:,1]\n",
    "# y_test = test_array[:,2:]\n",
    "\n",
    "\n",
    "twitter = Twitter()\n",
    "tmp = []\n",
    "max_count = 0\n",
    "set_words = set()\n",
    "for i,x in enumerate(x_val):\n",
    "    x = re.sub(r\"[^ㄱ-힣a-zA-Z0-9]+\", ' ', x).strip().split()\n",
    "# RuntimeError: No matching overloads found. at src/native/common/jp_method.cpp:121\n",
    "# 에러가 뜨는 경우에는 argument를 잘 확인해 주어야 한다. 현재의 경우 str이 들어가야 하는 자리에 list형식이 들어가있다.\n",
    "    tmp.append(twitter.pos(str(x), norm=True, stem=True))\n",
    "    \n",
    "#      if twitter.pos(str(x))[1] is not 'Punctuation')\n",
    "#     tmp = [s for s in tmp[i] if (s[1] != 'Punctuation' or s[1] != 'Alpha')]\n",
    "#     (b.append(a) if a is not None else None)\n",
    "    for j in tmp[i]:\n",
    "        set_words.add(j)\n",
    "    if len(tmp[i]) > max_count:\n",
    "        max_count = len(tmp[i])\n",
    "        if max_count == 93:\n",
    "            print(i)\n",
    "        \n",
    "print(max_count)\n",
    "    \n",
    "\n",
    "# # Tokenize\n",
    "# tokenizer = Tokenizer(num_words=max_features)\n",
    "# tokenizer.fit_on_texts(list(list_sentences))\n",
    "# list_tokenized_train = tokenizer.texts_to_sequences(list_sentences)\n",
    "\n",
    "# # Pad\n",
    "# x = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 너를 잃을 수 없다. 왜냐하면 내가 한 적이 있다면, 나는 내 가장 친한 친구, 내 영혼의 짝, 나의 미소, 내 웃음, 내 모든 것을 잃어 버렸기 때문이다.\n",
      "[(\"['\", 'Punctuation'), ('나', 'Noun'), ('는', 'Josa'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('너', 'Noun'), ('를', 'Josa'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('잃다', 'Verb'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('수', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('없다', 'Adjective'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('왜냐하면', 'Adverb'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('내', 'Noun'), ('가', 'Josa'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('하다', 'Verb'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('적', 'Noun'), ('이', 'Josa'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('있다', 'Adjective'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('나', 'Noun'), ('는', 'Josa'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('내', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('가장', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('친하다', 'Adjective'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('친구', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('내', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('영혼', 'Noun'), ('의', 'Josa'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('짝', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('나', 'Noun'), ('의', 'Josa'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('미소', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('내', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('웃음', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('내', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('모든', 'Noun'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('것', 'Noun'), ('을', 'Josa'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('잃다', 'Verb'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('버리다', 'Verb'), (\"',\", 'Punctuation'), (\"'\", 'Punctuation'), ('때문', 'Noun'), ('이다', 'Josa'), (\"']\", 'Punctuation')]\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "print(x_val[613])\n",
    "print(tmp[613])\n",
    "print(len(tmp[613]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/minwookje/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e5975d97231a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdummy_nltk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/minwookje/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "dummy_nltk = nltk.sent_tokenize(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert2Vec(self, model_name, doc):  ## Convert corpus into vectors\n",
    "#     word_vec = []\n",
    "#     model = gensim.models.word2vec.Word2Vec.load(model_name)\n",
    "    for sent in doc:\n",
    "        sub = []\n",
    "        for word in sent:\n",
    "            if(word in model.wv.vocab):\n",
    "                sub.append(model.wv[word])\n",
    "            else:\n",
    "                sub.append(np.random.uniform(-0.25,0.25,300)) ## used for OOV words\n",
    "        word_vec.append(sub)\n",
    "    \n",
    "    return np.array(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14241it [00:01, 8401.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14241 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# tmp1= 'a'\n",
    "# tmp2= 'a'\n",
    "# for i in a:\n",
    "#     tmp1 = i\n",
    "#     tmp2 = str(i)\n",
    "#     print(i,str(i))\n",
    "#     break\n",
    "# if str(tmp1) == tmp2:\n",
    "#     print(\"yes!\")\n",
    "#     쌍콤마 지우기 코드, 필요없을 수도 str씌우니까 dic_key타입이 \"\"사라지게 되는데\n",
    "# for i in :\n",
    "#     embeddings_index[i[1:-1]] = embeddings_index.pop(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "k = embeddings_index\n",
    "print(type(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.209341    0.0262855  -0.16035201  0.19948401  0.582286   -0.0345117\n",
      " -0.121298   -0.0912677  -0.51300198  0.27131999  0.0968003   0.0865747\n",
      "  0.17522299 -0.36303499 -0.319419    0.080253   -0.245655    0.170451\n",
      "  0.273516   -0.253815   -0.17695899  0.207268    0.276824    0.0145649\n",
      " -0.19889501  0.21917699  0.127717   -0.49855    -0.0957409  -0.91694498\n",
      "  0.0827512  -0.161661   -0.00689074 -0.0747906   0.27707499  0.00543082\n",
      "  0.15478399  0.181251   -0.44859901  0.18153299  0.628097   -0.0156562\n",
      "  0.088979    0.310588    0.124403   -0.080413    0.156323   -0.134864\n",
      "  0.23675799 -0.00917961  0.12836     0.29467499 -0.170986   -0.201645\n",
      " -0.00965752 -0.103357   -0.00537147  0.25499001 -0.0436398   0.25271299\n",
      "  0.058608   -0.134019    0.184654    0.46035799  0.0289141  -0.15943199\n",
      " -0.57392198 -0.38885701 -0.303141    0.12956899 -0.33357501 -0.0230292\n",
      " -0.25253001  0.34411901 -0.13720299 -0.0360019  -0.54765701  0.26774201\n",
      " -0.145886    0.0787636   0.106609    0.23292901 -0.126531   -0.14082\n",
      "  0.466961   -0.0342349   0.51228398 -0.102576   -0.336952    0.54568201\n",
      " -0.0424188  -0.29700699 -0.38751501 -0.328069   -0.21581601 -0.107457\n",
      " -0.363985    0.254832    0.0797203   0.38328201 -0.29948899  0.25117999\n",
      " -0.29400301  0.37006301 -0.040198    0.0861661  -0.0246104  -0.272163\n",
      "  0.108392    0.25155699  0.205607   -0.25090599  0.22819699  0.381726\n",
      "  0.27543601 -0.14674801 -0.0762573   0.0951494  -0.0715109  -0.150822\n",
      " -0.00231476 -0.289516   -0.178354   -0.0644636   0.39412001  0.29896399\n",
      " -0.18097401 -0.00688342  0.0515613   0.0185983  -0.0481196  -0.0101844\n",
      "  0.188527   -0.52174097 -0.27199599 -0.0539205  -0.275199   -0.0471367\n",
      "  0.388998   -0.526263   -0.0490241   0.0379806   0.80005002 -0.19707599\n",
      " -0.201125   -0.0294459   0.243111   -0.10603     0.0451783   0.556912\n",
      "  0.0016725  -0.026206   -0.236843    0.26869699  0.0465367  -0.0892428\n",
      "  0.0343101   0.298778   -0.0759662  -0.0519671  -0.60437298 -0.362688\n",
      "  0.341124   -0.0655884  -0.275161    0.43728799  0.169337    0.0894521\n",
      " -0.178689   -0.234808    0.0297143   0.089629   -0.0618781   0.15799201\n",
      "  0.065667    0.20175201  0.46819901 -0.242925    0.42630401  0.197143\n",
      " -0.147085    0.13721199  0.59695601 -0.0529921   0.059252    0.46260199\n",
      " -0.36325499  0.310112   -0.00899968  0.0591775  -0.17768    -0.0248718\n",
      " -0.25221699  0.16980299 -0.309044    0.193902   -0.159541    0.39214301\n",
      " -0.627976    0.36472201  0.0298297  -0.221527   -0.30800399  0.0524223\n",
      " -0.19015101  0.221265    0.0953278   0.200867    0.0266512  -0.0649767\n",
      " -0.44143599  0.123413   -0.26682201  0.0776222  -0.28013599 -0.251423\n",
      "  0.56232601 -0.18459301  0.301768    0.21377     0.102642    0.130308\n",
      "  0.0684142  -0.31577501  0.15579499  0.114361    0.0432752  -0.264633\n",
      " -0.0299315   0.26264799  0.25572199 -0.14264099 -0.0427615  -0.0623204\n",
      "  0.0453118   0.42638999  0.0856988  -0.0851914   0.28221199 -0.320342\n",
      " -0.0804786   0.0619573  -0.28438601  0.130124   -0.19839101 -0.31552801\n",
      " -0.00576089 -0.26221901 -0.0703881   0.0255654  -0.267759    0.0176885\n",
      " -0.236223   -0.59986299 -0.0456517   0.183504   -0.0689105  -0.0728174\n",
      " -0.0420118  -0.32487199 -0.0718238   0.245425   -0.0811007  -0.49871901\n",
      " -0.133123    0.297562    0.0144602   0.0116389  -0.100769    0.35409299\n",
      "  0.0814048   0.128224   -0.276887    0.17884099  0.177696   -0.208414\n",
      "  0.2983      0.262265    0.37759599 -0.231521    0.16758201 -0.162269\n",
      " -0.40867701 -0.48240599 -0.0998639   0.32235399 -0.47396401  0.43675101\n",
      " -0.230165   -0.185127    0.0230082  -0.299256    0.132164    0.0130504\n",
      "  0.26865801 -0.52312601 -0.202801   -0.14594001 -0.0113475  -0.21926901]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_index[\"('대통령','Noun')\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Tokenizer' has no attribute 'word_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-39b9db79db84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a weight matrix for words in training docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMB_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Tokenizer' has no attribute 'word_index'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14241it [00:01, 8448.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14241 word vectors.\n"
     ]
    }
   ],
   "source": [
    "f = codecs.open('/home/minwookje/coding/ex1_w2v/embedding/1542948553only_go_twitter_pos.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "# word2index table 생성\n",
    "# train 문장 token 갯수\n",
    "word_index = len(set_words)\n",
    "word_tmp_dict = dict()\n",
    "for i, word in enumerate(set_words):\n",
    "    word_tmp_dict[str(word).replace(\" \", \"\")] = i\n",
    "word_tmp_dict['0'] = word_index \n",
    "\n",
    "## 문장별 토큰화시킨 녀석에 index를 집어 넣어준다. 이때 pad도 동시에 해준다. \n",
    "\n",
    "word_vec = []\n",
    "for sent in tmp:\n",
    "    sub = []\n",
    "    for word in sent:\n",
    "         #print(word)\n",
    "         #print(type(str(word)))\n",
    "         #break\n",
    "# word는 tuple 타입, embeddings_index는 str타입, tuple 타입을 str()화시키면 \n",
    "# 중간에 space가 생성되어 match가 되지 않았다. 이를 해결해주었다. \n",
    "        if(str(word).replace(\" \", \"\") in word_tmp_dict):\n",
    "            \n",
    "            sub.append(word_tmp_dict[str(word).replace(\" \", \"\")])\n",
    "        else:\n",
    "            print(\"sentence index화 실패\")\n",
    "    count = max_count - len(sub)\n",
    "    sub.extend([word_index]*count)\n",
    "    word_vec.append(sub)\n",
    "\n",
    "\n",
    "# 4번쨰 matrix\n",
    "vocab_size = min(MAX_NB_WORDS, word_index)\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size+1, EMB_DIM))\n",
    "match_count = 0\n",
    "unmatch_count = 0\n",
    "\n",
    "for word, i in word_tmp_dict.items():\n",
    "    if word != '0':\n",
    "        if (word in embeddings_index):\n",
    "            match_count += 1\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            unmatch_count += 1\n",
    "            embedding_matrix[i] = np.random.uniform(-0.25,0.25,300) ## used for OOV words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     여기부터\n",
    "# 1.앞서 단어당 벡터 테이블(v) // embeddings_index, {w2v_word: vector}\n",
    "# train_word(str(word).replace(\" \", \"\")) == embedding\n",
    "# 2.train 단어별 index (v) // word_tmp_dict {train_word(str(word).replace(\" \", \"\")):index}\n",
    "# 3.sentence padding, sentence to index\n",
    "# 4.index당 vector table (x) //embedding_matrix {index: vector} 이녀석을 embedding weight에 넣어주어야 한다. \n",
    "\n",
    "# 문장 = [index들 나열 ] \n",
    "# 즉 embedding_matrix로 index를 seq에 넣어준묹장들을 train에 넣어줘야한다. \n",
    "\n",
    "\n",
    "# print(match_count, unmatch_count)\n",
    "# print(len(tmp))\n",
    "# print(x_val.shape)\n",
    "\n",
    "# x_val =  np.array(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vec[300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02440675,  0.10759468,  0.05138169,  0.02244159, -0.0381726 ,\n",
       "        0.07294706, -0.03120639,  0.1958865 ,  0.23183138, -0.05827924,\n",
       "        0.14586252,  0.01444746,  0.03402228,  0.21279832, -0.21448197,\n",
       "       -0.20643535, -0.2398908 ,  0.16630992,  0.13907838,  0.18500607,\n",
       "        0.23930917,  0.14957928, -0.01926032,  0.14026459, -0.19086279,\n",
       "        0.06996051, -0.17832336,  0.22233446,  0.01092416, -0.04266903,\n",
       "       -0.11772219,  0.13711684, -0.02192483,  0.03421697, -0.2406051 ,\n",
       "        0.05881775,  0.05604786,  0.058467  ,  0.22187404,  0.09091015,\n",
       "       -0.07024605, -0.03148402,  0.0988156 , -0.21988726,  0.08338336,\n",
       "        0.08531893, -0.14480872, -0.18553685, -0.09228582, -0.06814461,\n",
       "        0.03509839, -0.03069924,  0.24418692, -0.19897759, -0.14556162,\n",
       "       -0.16934524,  0.07655416, -0.1233542 , -0.01684461, -0.1277872 ,\n",
       "       -0.17051521, -0.19481243,  0.07816479, -0.18090852, -0.15170882,\n",
       "       -0.06563741,  0.16049661, -0.20144936,  0.16897245, -0.2019508 ,\n",
       "        0.23822973, -0.0156744 ,  0.23838054,  0.05242276,  0.11963179,\n",
       "       -0.2304061 , -0.10859652, -0.18990172, -0.1019299 , -0.19063614,\n",
       "       -0.09100841, -0.0428685 , -0.21792625,  0.09623606,  0.03330073,\n",
       "       -0.11730525,  0.01162403, -0.20302974,  0.03797325,  0.2146481 ,\n",
       "       -0.09071552,  0.08370519, -0.18410107,  0.1081636 , -0.10529695,\n",
       "       -0.15840432,  0.04325647, -0.23994623,  0.16447001, -0.24765226,\n",
       "        0.08890827, -0.11499601,  0.11759701,  0.23109427, -0.12562343,\n",
       "        0.03807867,  0.04602097,  0.03612595, -0.13845918,  0.22637451,\n",
       "       -0.02643731,  0.17320434,  0.09973964, -0.10128152,  0.15689891,\n",
       "       -0.05174713,  0.1905516 ,  0.04063644,  0.19086768,  0.0962658 ,\n",
       "        0.11262714,  0.00066219,  0.22804182,  0.0719951 , -0.03807248,\n",
       "        0.05319661, -0.2404034 , -0.09921259,  0.08008677, -0.1049612 ,\n",
       "        0.05900771, -0.03561565, -0.18226297, -0.10085884,  0.03498246,\n",
       "        0.04543638,  0.03716262,  0.07660041,  0.07605164, -0.03429078,\n",
       "        0.1982733 , -0.06621906, -0.03206754,  0.19596168,  0.15309699,\n",
       "        0.10194429, -0.19988656,  0.20974131,  0.10712065,  0.2494235 ,\n",
       "       -0.17527585,  0.18406303, -0.16875353,  0.05777978, -0.18809001,\n",
       "        0.17400411,  0.15365948,  0.03455037, -0.04640835, -0.2154165 ,\n",
       "        0.09871439, -0.02322866,  0.1110278 ,  0.18319116,  0.23776075,\n",
       "        0.17790167, -0.24414296, -0.07001097,  0.11499528, -0.16418516,\n",
       "        0.0105183 , -0.22283101, -0.15000174, -0.2407391 ,  0.14684885,\n",
       "       -0.13803766, -0.07732416,  0.21404065,  0.1022072 , -0.23408054,\n",
       "       -0.16765292,  0.0607392 ,  0.03861429, -0.13105359,  0.217107  ,\n",
       "        0.05698298,  0.0178164 ,  0.04495499,  0.11506101, -0.0940275 ,\n",
       "       -0.05088947, -0.14507813, -0.1569035 ,  0.22218619,  0.1197754 ,\n",
       "       -0.0047706 , -0.13629269, -0.12282176, -0.22098542, -0.03279169,\n",
       "       -0.09410206,  0.09817174, -0.06112408, -0.16019816, -0.23766064,\n",
       "       -0.21637518,  0.08969639, -0.02315158,  0.01828961,  0.19833565,\n",
       "        0.24516947, -0.14155151,  0.0815391 , -0.11833881, -0.2396745 ,\n",
       "        0.12918933, -0.08999142, -0.05826805,  0.04415856,  0.16552423,\n",
       "        0.06449092,  0.18632533, -0.11322898,  0.14902342, -0.15718203,\n",
       "        0.22639583,  0.09374414, -0.14224616,  0.2236853 ,  0.1154279 ,\n",
       "       -0.12302918, -0.14334401,  0.00910036, -0.23716864, -0.14626496,\n",
       "       -0.03765727, -0.06291501, -0.01821229, -0.11118565,  0.04339217,\n",
       "        0.1819278 , -0.19123407,  0.00868955, -0.18396595,  0.10842984,\n",
       "       -0.05197015,  0.03271066, -0.15836008, -0.17757612, -0.00597186,\n",
       "       -0.07219363,  0.22021597,  0.13266263,  0.12433181,  0.20185987,\n",
       "       -0.20828878,  0.02609623,  0.04223803,  0.23096819, -0.10392624,\n",
       "       -0.12958561, -0.19985303, -0.24178519,  0.21476466,  0.08495827,\n",
       "        0.14257646, -0.10913495,  0.04320508, -0.21802237, -0.0071862 ,\n",
       "        0.23874757,  0.18825262, -0.08092052,  0.23078508, -0.13414919,\n",
       "        0.22465941,  0.22068885,  0.14960129,  0.06522397,  0.18714398,\n",
       "       -0.10348986,  0.17447178,  0.05893835, -0.24338157, -0.07638324,\n",
       "       -0.17592957,  0.24091469, -0.01081485, -0.00130432,  0.06973626,\n",
       "       -0.0657077 , -0.18154986,  0.16105887, -0.15507604,  0.00565949,\n",
       "       -0.13784149, -0.20107776,  0.18109576,  0.23645974,  0.23041733])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a.extend([0]*4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [tmp[1]]+ [tmp[2]]\n",
    "# lists =[ ['9034968', 'ETH'], ['14160113', 'ETH'], ['9034968', 'ETH'], ['11111', 'NOT'], ['9555269', 'NOT'], ['15724032', 'ETH'], ['15481740', 'ETH'], ['15481757', 'ETH'], ['15481724', 'ETH'], ['10307528', 'ETH'], ['15481757', 'ETH'], ['15481724', 'ETH'], ['15481740', 'ETH'], ['15379365', 'ETH'], ['11111', 'NOT'], ['9555269', 'NOT'], ['15379365', 'ETH']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6c029f5b7221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple_e\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtuple_e\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist_x\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlist_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "unique_data = [x for x in set(tuple_e for tuple_e in (list_x for list_x in lists))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'b', 'c', '1'], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "# k= [list(x) for x in set(tuple(x) for x in [[1,2,3],['a','b','c','1']])]\n",
    "k = [list(x) for x in set(tuple(x) for x in [[1,2,3],['1','b','c','1']])]\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(lists))\n",
    "print(len(unique_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
